{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "SEED = 1234\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Found 1406 images belonging to 20 classes.\n",
      "\n",
      "Validation\n",
      "Found 148 images belonging to 20 classes.\n",
      "\n",
      "To predict\n",
      "Found 500 images.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "bs = 8\n",
    "img_w = 256\n",
    "img_h = 256\n",
    "validation_split = 0.1\n",
    "\n",
    "classes = [\n",
    "    'owl',              # 0\n",
    "    'galaxy',           # 1\n",
    "    'lightning',        # 2\n",
    "    'wine-bottle',      # 3\n",
    "    't-shirt',          # 4\n",
    "    'waterfall',        # 5\n",
    "    'sword',            # 6\n",
    "    'school-bus',       # 7\n",
    "    'calculator',       # 8\n",
    "    'sheet-music',      # 9\n",
    "    'airplanes',        # 10\n",
    "    'lightbulb',        # 11\n",
    "    'skyscraper',       # 12\n",
    "    'mountain-bike',    # 13\n",
    "    'fireworks',        # 14\n",
    "    'computer-monitor', # 15\n",
    "    'bear',             # 16\n",
    "    'grand-piano',      # 17\n",
    "    'kangaroo',         # 18\n",
    "    'laptop'            # 19\n",
    "]\n",
    "\n",
    "# LOAD TRAINING AND VALIDATION SETS\n",
    "\n",
    "data_gen = ImageDataGenerator(rotation_range = 10,\n",
    "                              width_shift_range = 10,\n",
    "                              height_shift_range = 10,\n",
    "                              zoom_range = 0.3,\n",
    "                              horizontal_flip = True,\n",
    "                              vertical_flip = True,\n",
    "                              rescale = 1./255, \n",
    "                              validation_split = validation_split)\n",
    "\n",
    "training_dir = os.path.join(cwd, \"Classification_Dataset\", \"training\")\n",
    "\n",
    "print(\"Training\")\n",
    "\n",
    "training_generator = data_gen.flow_from_directory(training_dir,\n",
    "                                        batch_size = bs,\n",
    "                                        classes = classes,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        shuffle = True,\n",
    "                                        seed = SEED,\n",
    "                                        subset = 'training')\n",
    "\n",
    "print(\"\\nValidation\")\n",
    "\n",
    "validation_generator = data_gen.flow_from_directory(training_dir,\n",
    "                                        batch_size = bs,\n",
    "                                        classes = classes,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        shuffle = True,\n",
    "                                        seed = SEED,\n",
    "                                        subset = 'validation')\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_generator(lambda: training_generator,\n",
    "                                              output_types = (tf.float32, tf.float32),\n",
    "                                              output_shapes = ([None, img_w, img_h, 3], [None, len(classes)]))\n",
    "validation_dataset = tf.data.Dataset.from_generator(lambda: validation_generator,\n",
    "                                              output_types = (tf.float32, tf.float32),\n",
    "                                              output_shapes = ([None, img_w, img_h, 3], [None, len(classes)]))\n",
    "\n",
    "training_dataset = training_dataset.repeat()\n",
    "validation_dataset = validation_dataset.repeat()\n",
    "\n",
    "# WRITE FILENAMES TO JSON FILE\n",
    "\n",
    "filenames = {\n",
    "    \"training\" : {},\n",
    "    \"validation\" : {}\n",
    "}\n",
    "\n",
    "for c in classes:\n",
    "    filenames[\"training\"][c] = []\n",
    "    filenames[\"validation\"][c] = []\n",
    "    \n",
    "    for fn in training_generator.filenames:\n",
    "        if c in fn:\n",
    "            filenames[\"training\"][c].append(fn.replace(c + \"/\", \"\"))\n",
    "    \n",
    "    for fn in validation_generator.filenames:\n",
    "        if c in fn:\n",
    "            filenames[\"validation\"][c].append(fn.replace(c + \"/\", \"\"))\n",
    "\n",
    "with open('dataset_split.json', 'w') as file:\n",
    "    json.dump(filenames, file, indent=4)\n",
    "    \n",
    "\n",
    "# LOAD TEST SET filenames\n",
    "\n",
    "print(\"\\nTo predict\")\n",
    "\n",
    "test_dir = os.path.join(cwd, \"Classification_Dataset\", \"test\")\n",
    "test_filenames = next(os.walk(test_dir))[2]\n",
    "test_filenames = list(filter(lambda fn: fn[-4:] == '.jpg', test_filenames))\n",
    "\n",
    "print(\"Found \" + str(len(test_filenames)) + \" images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building, fitting and predicting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, conv_depth, start_num_filters, kernel_size, pool_size, fc_units, num_classes, kernel_regularizer = None):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(conv_depth):\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(filters = start_num_filters,\n",
    "                             kernel_size = kernel_size,\n",
    "                             strides = (1, 1),\n",
    "                             padding = 'same',\n",
    "                             activation = 'relu',\n",
    "                             input_shape = input_shape))\n",
    "        else:\n",
    "            model.add(Conv2D(filters = start_num_filters,\n",
    "                             kernel_size = kernel_size,\n",
    "                             strides = (1, 1),\n",
    "                             padding = 'same',\n",
    "                             activation = 'relu'))\n",
    "            \n",
    "        model.add(MaxPool2D(pool_size = pool_size))\n",
    "        start_num_filters *= 2\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for i in range(len(fc_units)):\n",
    "        model.add(Dense(units = fc_units[i], \n",
    "                        activation = 'relu', \n",
    "                        kernel_regularizer = kernel_regularizer))\n",
    "    \n",
    "    model.add(Dense(units = num_classes, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(model, model_name = datetime.now().strftime('%b%d_%H-%M-%S')):\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # General experiments folder\n",
    "    exps_dir = os.path.join(cwd, 'classification_experiments')\n",
    "    if not os.path.exists(exps_dir):\n",
    "        os.makedirs(exps_dir)\n",
    "    \n",
    "    now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    \n",
    "    # This experiment folder\n",
    "    exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "    if not os.path.exists(exp_dir):\n",
    "        os.makedirs(exp_dir)\n",
    "    \n",
    "    # Checpoints folder\n",
    "    ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    \n",
    "    # Checkpoints callback, best one will be the last saved\n",
    "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                       save_weights_only=True,\n",
    "                                                       save_best_only=True, \n",
    "                                                       monitor='val_accuracy', \n",
    "                                                       mode = 'max')\n",
    "    \n",
    "    # Tensorboard folder\n",
    "    tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "    if not os.path.exists(tb_dir):\n",
    "        os.makedirs(tb_dir)\n",
    "    \n",
    "    # Tensorboard callback\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                                 profile_batch=0,\n",
    "                                                 histogram_freq=1)  # if 1 shows weights histograms\n",
    "    \n",
    "    # Early stopping callback\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                                   patience=10,\n",
    "                                                   mode = 'max')\n",
    "    \n",
    "    callbacks= [ckpt_callback, tb_callback, es_callback]\n",
    "    \n",
    "    model.fit(x=training_dataset,\n",
    "              epochs=100,\n",
    "              steps_per_epoch=len(training_generator),\n",
    "              validation_data=validation_dataset,\n",
    "              validation_steps=len(validation_generator), \n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Load best model (last one saved)\n",
    "    latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "    print(\"Latest model: \" + latest)\n",
    "    model.load_weights(os.path.join(ckpt_dir, latest))\n",
    "    \n",
    "    return (model, exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, exp_dir):\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for image_name in test_filenames:\n",
    "        img = Image.open(os.path.join(test_dir,image_name)).convert('RGB').resize((img_w, img_h))\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, 0)\n",
    "        img_array = np.divide(img_array,255)\n",
    "        tensor = tf.convert_to_tensor(img_array, dtype = tf.float32)\n",
    "        \n",
    "        prediction = np.argmax(model.predict(tensor))\n",
    "        results[image_name] = prediction\n",
    "\n",
    "    create_csv(results = results, results_dir=exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_1: as seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_name = \"CNN_1\"\n",
    "#conv_depth = 5\n",
    "#start_num_filters = 8\n",
    "#kernel_size = (3, 3)\n",
    "#pool_size = (2, 2)\n",
    "#fc_units = [512]\n",
    "#num_classes = len(classes)\n",
    "#\n",
    "#lr = 1e-3\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "#loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "#metrics = ['accuracy']\n",
    "#\n",
    "#model = build_model(input_shape = (img_w, img_h, 3),\n",
    "#                   conv_depth = conv_depth,\n",
    "#                   start_num_filters = start_num_filters,\n",
    "#                   kernel_size = kernel_size,\n",
    "#                   pool_size = pool_size,\n",
    "#                   fc_units = fc_units,\n",
    "#                   num_classes = num_classes)\n",
    "#\n",
    "#model.compile(optimizer = optimizer,\n",
    "#             loss = loss,\n",
    "#             metrics = metrics)\n",
    "#\n",
    "#model.summary()\n",
    "#\n",
    "#(model, exp_dir) = fit_model(model = model, model_name = model_name)\n",
    "#\n",
    "#predict(model = model, exp_dir = exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_2: as seen in class + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_name = \"CNN_2\"\n",
    "#conv_depth = 5\n",
    "#start_num_filters = 8\n",
    "#kernel_size = (3, 3)\n",
    "#pool_size = (2, 2)\n",
    "#fc_units = [512]\n",
    "#num_classes = len(classes)\n",
    "#\n",
    "#alpha = 0.01\n",
    "#kernel_regularizer = tf.keras.regularizers.l2(alpha)\n",
    "#\n",
    "#lr = 1e-3\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "#loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "#metrics = ['accuracy']\n",
    "#\n",
    "#model = build_model(input_shape = (img_w, img_h, 3),\n",
    "#                   conv_depth = conv_depth,\n",
    "#                   start_num_filters = start_num_filters,\n",
    "#                   kernel_size = kernel_size,\n",
    "#                   pool_size = pool_size,\n",
    "#                   fc_units = fc_units, \n",
    "#                   kernel_regularizer = kernel_regularizer,\n",
    "#                   num_classes = num_classes)\n",
    "#\n",
    "#model.compile(optimizer = optimizer,\n",
    "#             loss = loss,\n",
    "#             metrics = metrics)\n",
    "#\n",
    "#model.summary()\n",
    "#\n",
    "#(model, exp_dir) = fit_model(model = model, model_name = model_name)\n",
    "#\n",
    "#predict(model = model, exp_dir = exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_3: 7 layers, start_num_filters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 4)       112       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 8)       296       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 928,796\n",
      "Trainable params: 928,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 176 steps, validate for 19 steps\n",
      "Epoch 1/100\n",
      "176/176 [==============================] - 48s 273ms/step - loss: 2.9940 - accuracy: 0.0484 - val_loss: 2.9757 - val_accuracy: 0.0676\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 39s 222ms/step - loss: 2.9843 - accuracy: 0.0612 - val_loss: 2.9776 - val_accuracy: 0.0676\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 39s 220ms/step - loss: 2.9830 - accuracy: 0.0569 - val_loss: 2.9686 - val_accuracy: 0.0676\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 39s 220ms/step - loss: 2.9819 - accuracy: 0.0505 - val_loss: 2.9685 - val_accuracy: 0.0676\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 38s 218ms/step - loss: 2.9820 - accuracy: 0.0548 - val_loss: 2.9672 - val_accuracy: 0.0676\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 39s 219ms/step - loss: 2.9809 - accuracy: 0.0569 - val_loss: 2.9627 - val_accuracy: 0.0811\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 39s 221ms/step - loss: 2.9823 - accuracy: 0.0576 - val_loss: 2.9694 - val_accuracy: 0.0676\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 39s 222ms/step - loss: 2.9779 - accuracy: 0.0654 - val_loss: 2.9735 - val_accuracy: 0.0676\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 39s 219ms/step - loss: 2.9806 - accuracy: 0.0569 - val_loss: 2.9732 - val_accuracy: 0.0676\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 40s 226ms/step - loss: 2.9798 - accuracy: 0.0569 - val_loss: 2.9681 - val_accuracy: 0.0676\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 39s 220ms/step - loss: 2.9794 - accuracy: 0.0562 - val_loss: 2.9701 - val_accuracy: 0.0676\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 38s 216ms/step - loss: 2.9802 - accuracy: 0.0555 - val_loss: 2.9681 - val_accuracy: 0.0676\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 38s 217ms/step - loss: 2.9801 - accuracy: 0.0512 - val_loss: 2.9706 - val_accuracy: 0.0676\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 39s 222ms/step - loss: 2.9797 - accuracy: 0.0533 - val_loss: 2.9752 - val_accuracy: 0.0676\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 40s 225ms/step - loss: 2.9818 - accuracy: 0.0654 - val_loss: 2.9695 - val_accuracy: 0.0676\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 40s 228ms/step - loss: 2.9804 - accuracy: 0.0576 - val_loss: 2.9730 - val_accuracy: 0.0676\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\01 - Image Classification\\classification_experiments\\CNN_3_Nov19_20-16-26\\ckpts\\cp_06.ckpt\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"CNN_3\"\n",
    "#conv_depth = 7\n",
    "#start_num_filters = 4\n",
    "#kernel_size = (3, 3)\n",
    "#pool_size = (2, 2)\n",
    "#fc_units = [512]\n",
    "#num_classes = len(classes)\n",
    "#\n",
    "#lr = 1e-3\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "#loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "#metrics = ['accuracy']\n",
    "#\n",
    "#model = build_model(input_shape = (img_w, img_h, 3),\n",
    "#                   conv_depth = conv_depth,\n",
    "#                   start_num_filters = start_num_filters,\n",
    "#                   kernel_size = kernel_size,\n",
    "#                   pool_size = pool_size,\n",
    "#                   fc_units = fc_units,\n",
    "#                   num_classes = num_classes)\n",
    "#\n",
    "#model.compile(optimizer = optimizer,\n",
    "#             loss = loss,\n",
    "#             metrics = metrics)\n",
    "#\n",
    "#model.summary()\n",
    "#\n",
    "#(model, exp_dir) = fit_model(model = model, model_name = model_name)\n",
    "#\n",
    "#predict(model = model, exp_dir = exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_4: 7 layers, start_num_filters = 4, regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 256, 256, 4)       112       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 128, 128, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 128, 128, 8)       296       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 928,796\n",
      "Trainable params: 928,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 176 steps, validate for 19 steps\n",
      "Epoch 1/100\n",
      "176/176 [==============================] - 40s 226ms/step - loss: 3.7549 - accuracy: 0.0512 - val_loss: 2.9950 - val_accuracy: 0.0608\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 39s 224ms/step - loss: 2.9519 - accuracy: 0.0789 - val_loss: 2.9318 - val_accuracy: 0.1014\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 39s 223ms/step - loss: 2.8678 - accuracy: 0.0967 - val_loss: 2.7585 - val_accuracy: 0.1149\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 39s 222ms/step - loss: 2.7671 - accuracy: 0.1273 - val_loss: 2.7921 - val_accuracy: 0.1081\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 39s 224ms/step - loss: 2.6959 - accuracy: 0.1522 - val_loss: 2.6445 - val_accuracy: 0.1554\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 40s 226ms/step - loss: 2.6066 - accuracy: 0.1970 - val_loss: 2.5663 - val_accuracy: 0.1757\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 40s 225ms/step - loss: 2.5294 - accuracy: 0.2240 - val_loss: 2.4587 - val_accuracy: 0.2297\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 39s 221ms/step - loss: 2.4547 - accuracy: 0.2347 - val_loss: 2.4029 - val_accuracy: 0.2432\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 39s 224ms/step - loss: 2.3960 - accuracy: 0.2511 - val_loss: 2.3285 - val_accuracy: 0.2703\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 40s 226ms/step - loss: 2.3354 - accuracy: 0.2710 - val_loss: 2.3623 - val_accuracy: 0.3176\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 39s 221ms/step - loss: 2.2592 - accuracy: 0.3058 - val_loss: 2.1205 - val_accuracy: 0.3716\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 39s 222ms/step - loss: 2.1610 - accuracy: 0.3279 - val_loss: 2.2625 - val_accuracy: 0.3108\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 38s 218ms/step - loss: 2.1147 - accuracy: 0.3393 - val_loss: 2.1950 - val_accuracy: 0.2905\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 40s 225ms/step - loss: 2.0395 - accuracy: 0.3720 - val_loss: 2.2964 - val_accuracy: 0.3108\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 39s 221ms/step - loss: 2.0315 - accuracy: 0.3741 - val_loss: 2.1599 - val_accuracy: 0.3649\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 39s 220ms/step - loss: 1.9575 - accuracy: 0.4132 - val_loss: 2.0519 - val_accuracy: 0.3446\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 39s 224ms/step - loss: 1.8957 - accuracy: 0.4139 - val_loss: 2.0581 - val_accuracy: 0.3784\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 39s 223ms/step - loss: 1.8878 - accuracy: 0.4310 - val_loss: 1.9925 - val_accuracy: 0.3581\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 38s 217ms/step - loss: 1.7885 - accuracy: 0.4616 - val_loss: 2.0338 - val_accuracy: 0.3716\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 37s 211ms/step - loss: 1.7884 - accuracy: 0.4523 - val_loss: 1.9479 - val_accuracy: 0.4392\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 36s 207ms/step - loss: 1.7520 - accuracy: 0.4737 - val_loss: 1.9256 - val_accuracy: 0.4595\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - 36s 206ms/step - loss: 1.6856 - accuracy: 0.4922 - val_loss: 1.9255 - val_accuracy: 0.4730\n",
      "Epoch 23/100\n",
      "176/176 [==============================] - 37s 211ms/step - loss: 1.6689 - accuracy: 0.5057 - val_loss: 2.0407 - val_accuracy: 0.4189\n",
      "Epoch 24/100\n",
      "176/176 [==============================] - 41s 231ms/step - loss: 1.6445 - accuracy: 0.4886 - val_loss: 1.9515 - val_accuracy: 0.3919\n",
      "Epoch 25/100\n",
      "176/176 [==============================] - 40s 225ms/step - loss: 1.6106 - accuracy: 0.5028 - val_loss: 1.7736 - val_accuracy: 0.4527\n",
      "Epoch 26/100\n",
      "176/176 [==============================] - 36s 205ms/step - loss: 1.5584 - accuracy: 0.5363 - val_loss: 2.0040 - val_accuracy: 0.4459\n",
      "Epoch 27/100\n",
      "176/176 [==============================] - 36s 207ms/step - loss: 1.5580 - accuracy: 0.5341 - val_loss: 1.8341 - val_accuracy: 0.4865\n",
      "Epoch 28/100\n",
      "176/176 [==============================] - 36s 206ms/step - loss: 1.4994 - accuracy: 0.5519 - val_loss: 1.7308 - val_accuracy: 0.5405\n",
      "Epoch 29/100\n",
      "176/176 [==============================] - 36s 205ms/step - loss: 1.4844 - accuracy: 0.5619 - val_loss: 1.7964 - val_accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "176/176 [==============================] - 36s 205ms/step - loss: 1.4844 - accuracy: 0.5747 - val_loss: 1.7979 - val_accuracy: 0.4257\n",
      "Epoch 31/100\n",
      "176/176 [==============================] - 36s 205ms/step - loss: 1.4504 - accuracy: 0.5789 - val_loss: 1.9568 - val_accuracy: 0.4865\n",
      "Epoch 32/100\n",
      "176/176 [==============================] - 36s 206ms/step - loss: 1.4247 - accuracy: 0.5725 - val_loss: 1.8345 - val_accuracy: 0.5068\n",
      "Epoch 33/100\n",
      "176/176 [==============================] - 38s 216ms/step - loss: 1.4715 - accuracy: 0.5683 - val_loss: 1.8794 - val_accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "176/176 [==============================] - 37s 208ms/step - loss: 1.3964 - accuracy: 0.5868 - val_loss: 1.8946 - val_accuracy: 0.5203\n",
      "Epoch 35/100\n",
      "176/176 [==============================] - 40s 225ms/step - loss: 1.3524 - accuracy: 0.6131 - val_loss: 1.9139 - val_accuracy: 0.4730\n",
      "Epoch 36/100\n",
      "176/176 [==============================] - 40s 227ms/step - loss: 1.3701 - accuracy: 0.6003 - val_loss: 1.8517 - val_accuracy: 0.4865\n",
      "Epoch 37/100\n",
      "176/176 [==============================] - 40s 227ms/step - loss: 1.3580 - accuracy: 0.6031 - val_loss: 1.7708 - val_accuracy: 0.5135\n",
      "Epoch 38/100\n",
      "176/176 [==============================] - 40s 229ms/step - loss: 1.3672 - accuracy: 0.6117 - val_loss: 1.8714 - val_accuracy: 0.5338\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\01 - Image Classification\\classification_experiments\\CNN_4_Nov19_20-27-16\\ckpts\\cp_28.ckpt\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"CNN_4\"\n",
    "#conv_depth = 7\n",
    "#start_num_filters = 4\n",
    "#kernel_size = (3, 3)\n",
    "#pool_size = (2, 2)\n",
    "#fc_units = [512]\n",
    "#num_classes = len(classes)\n",
    "#\n",
    "#alpha = 0.01\n",
    "#kernel_regularizer = tf.keras.regularizers.l2(alpha)\n",
    "#\n",
    "#lr = 1e-3\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "#loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "#metrics = ['accuracy']\n",
    "#\n",
    "#model = build_model(input_shape = (img_w, img_h, 3),\n",
    "#                   conv_depth = conv_depth,\n",
    "#                   start_num_filters = start_num_filters,\n",
    "#                   kernel_size = kernel_size,\n",
    "#                   pool_size = pool_size,\n",
    "#                   fc_units = fc_units, \n",
    "#                   kernel_regularizer = kernel_regularizer,\n",
    "#                   num_classes = num_classes)\n",
    "#\n",
    "#model.compile(optimizer = optimizer,\n",
    "#             loss = loss,\n",
    "#             metrics = metrics)\n",
    "#\n",
    "#model.summary()\n",
    "#\n",
    "#(model, exp_dir) = fit_model(model = model, model_name = model_name)\n",
    "#\n",
    "#predict(model = model, exp_dir = exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_5: 6 layers, start_num_filters = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 256, 256, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 128, 128, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 128, 128, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 64, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 2,501,476\n",
      "Trainable params: 2,501,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 176 steps, validate for 19 steps\n",
      "Epoch 1/100\n",
      "176/176 [==============================] - 49s 280ms/step - loss: 2.9828 - accuracy: 0.0640 - val_loss: 2.9167 - val_accuracy: 0.0338\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 49s 277ms/step - loss: 2.8018 - accuracy: 0.1174 - val_loss: 2.6701 - val_accuracy: 0.1554\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 49s 278ms/step - loss: 2.6479 - accuracy: 0.1657 - val_loss: 2.5305 - val_accuracy: 0.1824\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 51s 289ms/step - loss: 2.4261 - accuracy: 0.2447 - val_loss: 2.2559 - val_accuracy: 0.2973\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 47s 269ms/step - loss: 2.2846 - accuracy: 0.2688 - val_loss: 2.2856 - val_accuracy: 0.2973\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 47s 266ms/step - loss: 2.1429 - accuracy: 0.3250 - val_loss: 2.1362 - val_accuracy: 0.3176\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 46s 260ms/step - loss: 2.0806 - accuracy: 0.3307 - val_loss: 2.2276 - val_accuracy: 0.3446\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 46s 261ms/step - loss: 1.9553 - accuracy: 0.3848 - val_loss: 1.9681 - val_accuracy: 0.3919\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 46s 259ms/step - loss: 1.8945 - accuracy: 0.3933 - val_loss: 1.8357 - val_accuracy: 0.4122\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 46s 261ms/step - loss: 1.8363 - accuracy: 0.4395 - val_loss: 1.8018 - val_accuracy: 0.4459\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 47s 266ms/step - loss: 1.7472 - accuracy: 0.4403 - val_loss: 1.8376 - val_accuracy: 0.4122\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 46s 264ms/step - loss: 1.6895 - accuracy: 0.4666 - val_loss: 1.9008 - val_accuracy: 0.3716\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 46s 261ms/step - loss: 1.5888 - accuracy: 0.4900 - val_loss: 1.7952 - val_accuracy: 0.3851\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 47s 269ms/step - loss: 1.5674 - accuracy: 0.4993 - val_loss: 1.6391 - val_accuracy: 0.4662\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 46s 260ms/step - loss: 1.4758 - accuracy: 0.5334 - val_loss: 1.7062 - val_accuracy: 0.4459\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 47s 268ms/step - loss: 1.4416 - accuracy: 0.5448 - val_loss: 1.7288 - val_accuracy: 0.4392\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 46s 262ms/step - loss: 1.3529 - accuracy: 0.5747 - val_loss: 1.6294 - val_accuracy: 0.4662\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 45s 254ms/step - loss: 1.2736 - accuracy: 0.6010 - val_loss: 1.8235 - val_accuracy: 0.4730\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 45s 258ms/step - loss: 1.2600 - accuracy: 0.6038 - val_loss: 1.6629 - val_accuracy: 0.5270\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 49s 280ms/step - loss: 1.2360 - accuracy: 0.6010 - val_loss: 1.8027 - val_accuracy: 0.4730\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 48s 271ms/step - loss: 1.1495 - accuracy: 0.6380 - val_loss: 1.7370 - val_accuracy: 0.4392\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - 48s 271ms/step - loss: 1.1279 - accuracy: 0.6344 - val_loss: 1.8172 - val_accuracy: 0.5068\n",
      "Epoch 23/100\n",
      "176/176 [==============================] - 46s 263ms/step - loss: 1.0993 - accuracy: 0.6593 - val_loss: 1.9421 - val_accuracy: 0.4392\n",
      "Epoch 24/100\n",
      "176/176 [==============================] - 46s 264ms/step - loss: 1.0343 - accuracy: 0.6757 - val_loss: 1.6776 - val_accuracy: 0.4932\n",
      "Epoch 25/100\n",
      "176/176 [==============================] - 46s 261ms/step - loss: 0.9971 - accuracy: 0.6871 - val_loss: 1.7212 - val_accuracy: 0.5270\n",
      "Epoch 26/100\n",
      "176/176 [==============================] - 46s 263ms/step - loss: 0.9884 - accuracy: 0.6878 - val_loss: 1.6696 - val_accuracy: 0.5270\n",
      "Epoch 27/100\n",
      "176/176 [==============================] - 47s 269ms/step - loss: 0.9394 - accuracy: 0.7063 - val_loss: 1.7451 - val_accuracy: 0.4865\n",
      "Epoch 28/100\n",
      "176/176 [==============================] - 46s 264ms/step - loss: 0.9279 - accuracy: 0.7141 - val_loss: 1.8811 - val_accuracy: 0.4932\n",
      "Epoch 29/100\n",
      "176/176 [==============================] - 47s 267ms/step - loss: 0.8939 - accuracy: 0.7134 - val_loss: 1.8181 - val_accuracy: 0.4730\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\01 - Image Classification\\classification_experiments\\CNN_5_Nov19_20-51-53\\ckpts\\cp_19.ckpt\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"CNN_5\"\n",
    "#conv_depth = 6\n",
    "#start_num_filters = 8\n",
    "#kernel_size = (3, 3)\n",
    "#pool_size = (2, 2)\n",
    "#fc_units = [512]\n",
    "#num_classes = len(classes)\n",
    "#\n",
    "#lr = 1e-3\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "#loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "#metrics = ['accuracy']\n",
    "#\n",
    "#model = build_model(input_shape = (img_w, img_h, 3),\n",
    "#                   conv_depth = conv_depth,\n",
    "#                   start_num_filters = start_num_filters,\n",
    "#                   kernel_size = kernel_size,\n",
    "#                   pool_size = pool_size,\n",
    "#                   fc_units = fc_units,\n",
    "#                   num_classes = num_classes)\n",
    "#\n",
    "#model.compile(optimizer = optimizer,\n",
    "#             loss = loss,\n",
    "#             metrics = metrics)\n",
    "#\n",
    "#model.summary()\n",
    "#\n",
    "#(model, exp_dir) = fit_model(model = model, model_name = model_name)\n",
    "#\n",
    "#predict(model = model, exp_dir = exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_6: 6 layers, start_num_filters = 8, regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 256, 256, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 128, 128, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 128, 128, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 64, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 2,501,476\n",
      "Trainable params: 2,501,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 176 steps, validate for 19 steps\n",
      "Epoch 1/100\n",
      "176/176 [==============================] - 51s 291ms/step - loss: 3.6141 - accuracy: 0.0647 - val_loss: 2.9912 - val_accuracy: 0.0676\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 50s 285ms/step - loss: 2.9883 - accuracy: 0.0640 - val_loss: 2.9789 - val_accuracy: 0.0676\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 49s 276ms/step - loss: 2.9825 - accuracy: 0.0526 - val_loss: 2.9751 - val_accuracy: 0.0676\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 50s 283ms/step - loss: 2.9812 - accuracy: 0.0484 - val_loss: 2.9713 - val_accuracy: 0.0676\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 50s 283ms/step - loss: 2.9801 - accuracy: 0.0505 - val_loss: 2.9655 - val_accuracy: 0.0676\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 50s 286ms/step - loss: 2.9809 - accuracy: 0.0597 - val_loss: 2.9721 - val_accuracy: 0.0676\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 50s 282ms/step - loss: 2.9797 - accuracy: 0.0583 - val_loss: 2.9712 - val_accuracy: 0.0676\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 49s 279ms/step - loss: 2.9803 - accuracy: 0.0576 - val_loss: 2.9697 - val_accuracy: 0.0676\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 49s 278ms/step - loss: 2.9796 - accuracy: 0.0541 - val_loss: 2.9674 - val_accuracy: 0.0676\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 49s 278ms/step - loss: 2.9804 - accuracy: 0.0555 - val_loss: 2.9674 - val_accuracy: 0.0676\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 49s 279ms/step - loss: 2.9794 - accuracy: 0.0526 - val_loss: 2.9695 - val_accuracy: 0.0676\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\01 - Image Classification\\classification_experiments\\CNN_6_Nov19_21-14-52\\ckpts\\cp_01.ckpt\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"CNN_6\"\n",
    "#conv_depth = 6\n",
    "#start_num_filters = 8\n",
    "#kernel_size = (3, 3)\n",
    "#pool_size = (2, 2)\n",
    "#fc_units = [512]\n",
    "#num_classes = len(classes)\n",
    "#\n",
    "#alpha = 0.01\n",
    "#kernel_regularizer = tf.keras.regularizers.l2(alpha)\n",
    "#\n",
    "#lr = 1e-3\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "#loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "#metrics = ['accuracy']\n",
    "#\n",
    "#model = build_model(input_shape = (img_w, img_h, 3),\n",
    "#                   conv_depth = conv_depth,\n",
    "#                   start_num_filters = start_num_filters,\n",
    "#                   kernel_size = kernel_size,\n",
    "#                   pool_size = pool_size,\n",
    "#                   fc_units = fc_units, \n",
    "#                   kernel_regularizer = kernel_regularizer,\n",
    "#                   num_classes = num_classes)\n",
    "#\n",
    "#model.compile(optimizer = optimizer,\n",
    "#             loss = loss,\n",
    "#             metrics = metrics)\n",
    "#\n",
    "#model.summary()\n",
    "#\n",
    "#(model, exp_dir) = fit_model(model = model, model_name = model_name)\n",
    "#\n",
    "#predict(model = model, exp_dir = exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_7: as seen in class, start_num_filters = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_26 (Conv2D)           (None, 256, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               8389120   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 8,791,988\n",
      "Trainable params: 8,791,988\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 176 steps, validate for 19 steps\n",
      "Epoch 1/100\n",
      "176/176 [==============================] - 93s 530ms/step - loss: 2.9238 - accuracy: 0.0868 - val_loss: 2.7866 - val_accuracy: 0.1014\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 92s 525ms/step - loss: 2.6633 - accuracy: 0.1586 - val_loss: 2.6552 - val_accuracy: 0.1554\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 90s 510ms/step - loss: 2.5175 - accuracy: 0.2183 - val_loss: 2.3321 - val_accuracy: 0.2027\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 90s 511ms/step - loss: 2.3144 - accuracy: 0.2646 - val_loss: 2.2578 - val_accuracy: 0.3041\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 92s 521ms/step - loss: 2.2118 - accuracy: 0.3208 - val_loss: 2.1958 - val_accuracy: 0.3108\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 91s 516ms/step - loss: 2.0599 - accuracy: 0.3578 - val_loss: 1.8925 - val_accuracy: 0.4054\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 89s 508ms/step - loss: 1.9662 - accuracy: 0.3962 - val_loss: 1.9155 - val_accuracy: 0.4054\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 89s 507ms/step - loss: 1.8787 - accuracy: 0.4260 - val_loss: 2.0108 - val_accuracy: 0.4054\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 92s 525ms/step - loss: 1.8088 - accuracy: 0.4339 - val_loss: 1.8318 - val_accuracy: 0.4324\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 93s 528ms/step - loss: 1.7108 - accuracy: 0.4623 - val_loss: 1.7558 - val_accuracy: 0.4392\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 94s 532ms/step - loss: 1.6094 - accuracy: 0.4929 - val_loss: 1.8110 - val_accuracy: 0.3919\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 95s 538ms/step - loss: 1.5160 - accuracy: 0.5178 - val_loss: 1.8084 - val_accuracy: 0.4595\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 94s 534ms/step - loss: 1.4511 - accuracy: 0.5384 - val_loss: 1.5482 - val_accuracy: 0.4797\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 96s 548ms/step - loss: 1.3920 - accuracy: 0.5512 - val_loss: 1.7683 - val_accuracy: 0.4662\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 95s 539ms/step - loss: 1.3267 - accuracy: 0.5832 - val_loss: 1.7342 - val_accuracy: 0.4865\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 95s 539ms/step - loss: 1.2291 - accuracy: 0.6138 - val_loss: 1.6426 - val_accuracy: 0.5135\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 95s 539ms/step - loss: 1.2140 - accuracy: 0.6074 - val_loss: 1.5783 - val_accuracy: 0.5338\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 95s 538ms/step - loss: 1.1571 - accuracy: 0.6358 - val_loss: 1.5763 - val_accuracy: 0.5608\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 94s 535ms/step - loss: 1.1358 - accuracy: 0.6501 - val_loss: 1.7539 - val_accuracy: 0.5473\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 93s 529ms/step - loss: 1.0740 - accuracy: 0.6558 - val_loss: 1.6866 - val_accuracy: 0.5135\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 93s 530ms/step - loss: 1.0577 - accuracy: 0.6814 - val_loss: 1.6096 - val_accuracy: 0.5135\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - 95s 537ms/step - loss: 0.9589 - accuracy: 0.6935 - val_loss: 1.7172 - val_accuracy: 0.4932\n",
      "Epoch 23/100\n",
      "176/176 [==============================] - 94s 534ms/step - loss: 0.9312 - accuracy: 0.6935 - val_loss: 1.7624 - val_accuracy: 0.5811\n",
      "Epoch 24/100\n",
      "176/176 [==============================] - 93s 527ms/step - loss: 0.8933 - accuracy: 0.7219 - val_loss: 1.7529 - val_accuracy: 0.5473\n",
      "Epoch 25/100\n",
      "176/176 [==============================] - 93s 529ms/step - loss: 0.8595 - accuracy: 0.7198 - val_loss: 1.6265 - val_accuracy: 0.5608\n",
      "Epoch 26/100\n",
      "176/176 [==============================] - 93s 528ms/step - loss: 0.8688 - accuracy: 0.7219 - val_loss: 1.7155 - val_accuracy: 0.5608\n",
      "Epoch 27/100\n",
      "176/176 [==============================] - 93s 530ms/step - loss: 0.8429 - accuracy: 0.7326 - val_loss: 1.6571 - val_accuracy: 0.5338\n",
      "Epoch 28/100\n",
      "176/176 [==============================] - 95s 538ms/step - loss: 0.7408 - accuracy: 0.7504 - val_loss: 1.9213 - val_accuracy: 0.5270\n",
      "Epoch 29/100\n",
      "176/176 [==============================] - 93s 529ms/step - loss: 0.7347 - accuracy: 0.7461 - val_loss: 1.7074 - val_accuracy: 0.5743\n",
      "Epoch 30/100\n",
      "176/176 [==============================] - 93s 530ms/step - loss: 0.7645 - accuracy: 0.7432 - val_loss: 1.5977 - val_accuracy: 0.5473\n",
      "Epoch 31/100\n",
      "176/176 [==============================] - 94s 534ms/step - loss: 0.7126 - accuracy: 0.7681 - val_loss: 1.5137 - val_accuracy: 0.6149\n",
      "Epoch 32/100\n",
      "176/176 [==============================] - 93s 528ms/step - loss: 0.6649 - accuracy: 0.7838 - val_loss: 1.8172 - val_accuracy: 0.5405\n",
      "Epoch 33/100\n",
      "176/176 [==============================] - 93s 529ms/step - loss: 0.6512 - accuracy: 0.7881 - val_loss: 1.9947 - val_accuracy: 0.4865\n",
      "Epoch 34/100\n",
      "176/176 [==============================] - 93s 527ms/step - loss: 0.6197 - accuracy: 0.7994 - val_loss: 1.9924 - val_accuracy: 0.4730\n",
      "Epoch 35/100\n",
      "176/176 [==============================] - 93s 527ms/step - loss: 0.5845 - accuracy: 0.8023 - val_loss: 2.2460 - val_accuracy: 0.4730\n",
      "Epoch 36/100\n",
      "176/176 [==============================] - 93s 527ms/step - loss: 0.6210 - accuracy: 0.7888 - val_loss: 1.9412 - val_accuracy: 0.5270\n",
      "Epoch 37/100\n",
      "176/176 [==============================] - 93s 527ms/step - loss: 0.5334 - accuracy: 0.8243 - val_loss: 2.2428 - val_accuracy: 0.5203\n",
      "Epoch 38/100\n",
      "176/176 [==============================] - 93s 527ms/step - loss: 0.5447 - accuracy: 0.8293 - val_loss: 1.9768 - val_accuracy: 0.5676\n",
      "Epoch 39/100\n",
      "176/176 [==============================] - 93s 528ms/step - loss: 0.5140 - accuracy: 0.8279 - val_loss: 2.2114 - val_accuracy: 0.5676\n",
      "Epoch 40/100\n",
      "176/176 [==============================] - 93s 529ms/step - loss: 0.5078 - accuracy: 0.8343 - val_loss: 2.7840 - val_accuracy: 0.5000\n",
      "Epoch 41/100\n",
      "176/176 [==============================] - 93s 528ms/step - loss: 0.5219 - accuracy: 0.8265 - val_loss: 2.2916 - val_accuracy: 0.5743\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\01 - Image Classification\\classification_experiments\\CNN_7_Nov19_21-29-04\\ckpts\\cp_31.ckpt\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"CNN_7\"\n",
    "#conv_depth = 5\n",
    "#start_num_filters = 16\n",
    "#kernel_size = (3, 3)\n",
    "#pool_size = (2, 2)\n",
    "#fc_units = [512]\n",
    "#num_classes = len(classes)\n",
    "#\n",
    "#lr = 1e-3\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "#loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "#metrics = ['accuracy']\n",
    "#\n",
    "#model = build_model(input_shape = (img_w, img_h, 3),\n",
    "#                   conv_depth = conv_depth,\n",
    "#                   start_num_filters = start_num_filters,\n",
    "#                   kernel_size = kernel_size,\n",
    "#                   pool_size = pool_size,\n",
    "#                   fc_units = fc_units,\n",
    "#                   num_classes = num_classes)\n",
    "#\n",
    "#model.compile(optimizer = optimizer,\n",
    "#             loss = loss,\n",
    "#             metrics = metrics)\n",
    "#\n",
    "#model.summary()\n",
    "#\n",
    "#(model, exp_dir) = fit_model(model = model, model_name = model_name)\n",
    "#\n",
    "#predict(model = model, exp_dir = exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_8: as seen in class, start_num_filters = 16, regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 256, 256, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 128, 128, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 128, 128, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 64, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 4,303,460\n",
      "Trainable params: 4,303,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 176 steps, validate for 19 steps\n",
      "Epoch 1/100\n",
      "176/176 [==============================] - 75s 429ms/step - loss: 3.5984 - accuracy: 0.0697 - val_loss: 2.8314 - val_accuracy: 0.1216\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 72s 409ms/step - loss: 2.8461 - accuracy: 0.1081 - val_loss: 2.7159 - val_accuracy: 0.1622\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 72s 409ms/step - loss: 2.7549 - accuracy: 0.1387 - val_loss: 2.6980 - val_accuracy: 0.1554\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 73s 413ms/step - loss: 2.6744 - accuracy: 0.1892 - val_loss: 2.6636 - val_accuracy: 0.1689\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 75s 425ms/step - loss: 2.5926 - accuracy: 0.2333 - val_loss: 2.5397 - val_accuracy: 0.2568\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 77s 436ms/step - loss: 2.4559 - accuracy: 0.2681 - val_loss: 2.3883 - val_accuracy: 0.2838\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 76s 433ms/step - loss: 2.3660 - accuracy: 0.3065 - val_loss: 2.3308 - val_accuracy: 0.3041\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 76s 431ms/step - loss: 2.3027 - accuracy: 0.3236 - val_loss: 2.3401 - val_accuracy: 0.3851\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 76s 431ms/step - loss: 2.2145 - accuracy: 0.3549 - val_loss: 2.1755 - val_accuracy: 0.3919\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 76s 431ms/step - loss: 2.1487 - accuracy: 0.3848 - val_loss: 2.3658 - val_accuracy: 0.3446\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 76s 432ms/step - loss: 2.1129 - accuracy: 0.3990 - val_loss: 2.0819 - val_accuracy: 0.4459\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 77s 439ms/step - loss: 2.0557 - accuracy: 0.4296 - val_loss: 2.1203 - val_accuracy: 0.4189\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 79s 446ms/step - loss: 2.0087 - accuracy: 0.4267 - val_loss: 2.1487 - val_accuracy: 0.4392\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 76s 432ms/step - loss: 1.9372 - accuracy: 0.4609 - val_loss: 2.1851 - val_accuracy: 0.4189\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 76s 430ms/step - loss: 1.9313 - accuracy: 0.4481 - val_loss: 2.0539 - val_accuracy: 0.4324\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 76s 435ms/step - loss: 1.8826 - accuracy: 0.4637 - val_loss: 2.2468 - val_accuracy: 0.4257\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 76s 434ms/step - loss: 1.9504 - accuracy: 0.4580 - val_loss: 2.1747 - val_accuracy: 0.3986\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 76s 431ms/step - loss: 1.8252 - accuracy: 0.4908 - val_loss: 2.0322 - val_accuracy: 0.4257\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 76s 430ms/step - loss: 1.8432 - accuracy: 0.4900 - val_loss: 1.9569 - val_accuracy: 0.4527\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 76s 429ms/step - loss: 1.7923 - accuracy: 0.5164 - val_loss: 2.0516 - val_accuracy: 0.4865\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 78s 445ms/step - loss: 1.7642 - accuracy: 0.5263 - val_loss: 2.1454 - val_accuracy: 0.4257\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - 76s 429ms/step - loss: 1.7625 - accuracy: 0.5107 - val_loss: 2.0017 - val_accuracy: 0.4595\n",
      "Epoch 23/100\n",
      "176/176 [==============================] - 77s 437ms/step - loss: 1.7419 - accuracy: 0.5427 - val_loss: 2.3028 - val_accuracy: 0.3919\n",
      "Epoch 24/100\n",
      "176/176 [==============================] - 75s 425ms/step - loss: 1.7330 - accuracy: 0.5391 - val_loss: 1.8921 - val_accuracy: 0.4865\n",
      "Epoch 25/100\n",
      "176/176 [==============================] - 75s 427ms/step - loss: 1.7032 - accuracy: 0.5377 - val_loss: 2.0533 - val_accuracy: 0.4122\n",
      "Epoch 26/100\n",
      "176/176 [==============================] - 74s 423ms/step - loss: 1.6623 - accuracy: 0.5541 - val_loss: 1.9176 - val_accuracy: 0.4730\n",
      "Epoch 27/100\n",
      "176/176 [==============================] - 75s 424ms/step - loss: 1.6651 - accuracy: 0.5548 - val_loss: 1.9856 - val_accuracy: 0.4797\n",
      "Epoch 28/100\n",
      "176/176 [==============================] - 75s 425ms/step - loss: 1.6630 - accuracy: 0.5590 - val_loss: 1.9345 - val_accuracy: 0.4595\n",
      "Epoch 29/100\n",
      "176/176 [==============================] - 75s 428ms/step - loss: 1.6855 - accuracy: 0.5661 - val_loss: 2.1139 - val_accuracy: 0.4865\n",
      "Epoch 30/100\n",
      "176/176 [==============================] - 75s 424ms/step - loss: 1.6264 - accuracy: 0.5661 - val_loss: 2.1410 - val_accuracy: 0.4865\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\01 - Image Classification\\classification_experiments\\CNN_8_Nov19_22-41-00\\ckpts\\cp_20.ckpt\n"
     ]
    }
   ],
   "source": [
    "model_name = \"CNN_8\"\n",
    "conv_depth = 5\n",
    "start_num_filters = 8\n",
    "kernel_size = (3, 3)\n",
    "pool_size = (2, 2)\n",
    "fc_units = [512]\n",
    "num_classes = len(classes)\n",
    "\n",
    "alpha = 0.01\n",
    "kernel_regularizer = tf.keras.regularizers.l2(alpha)\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model = build_model(input_shape = (img_w, img_h, 3),\n",
    "                   conv_depth = conv_depth,\n",
    "                   start_num_filters = start_num_filters,\n",
    "                   kernel_size = kernel_size,\n",
    "                   pool_size = pool_size,\n",
    "                   fc_units = fc_units, \n",
    "                   kernel_regularizer = kernel_regularizer,\n",
    "                   num_classes = num_classes)\n",
    "\n",
    "model.compile(optimizer = optimizer,\n",
    "             loss = loss,\n",
    "             metrics = metrics)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "(model, exp_dir) = fit_model(model = model, model_name = model_name)\n",
    "\n",
    "predict(model = model, exp_dir = exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
