{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "SEED = 1234\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Found 1406 images belonging to 20 classes.\n",
      "\n",
      "Validation\n",
      "Found 148 images belonging to 20 classes.\n",
      "\n",
      "To predict\n",
      "Found 501 images.\n",
      "['IMG_329.jpg', 'IMG_473.jpg', 'IMG_467.jpg', 'IMG_1322.jpg', 'IMG_1444.jpg', 'IMG_117.jpg', 'IMG_1849.jpg', 'IMG_671.jpg', 'IMG_1646.jpg', 'IMG_739.jpg', 'IMG_1901.jpg', 'IMG_1915.jpg', 'IMG_705.jpg', 'IMG_711.jpg', 'IMG_1929.jpg', 'IMG_1097.jpg', 'IMG_922.jpg', 'IMG_249.jpg', 'IMG_507.jpg', 'IMG_261.jpg', 'IMG_513.jpg', 'IMG_2005.jpg', 'IMG_1242.jpg', 'IMG_2010.jpg', 'IMG_512.jpg', 'IMG_0.jpg', 'IMG_1280.jpg', 'IMG_923.jpg', 'IMG_710.jpg', 'IMG_1684.jpg', 'IMG_664.jpg', 'IMG_880.jpg', 'IMG_499.jpg', 'IMG_1484.jpg', 'IMG_13.jpg', 'IMG_1645.jpg', 'IMG_1137.jpg', 'IMG_1902.jpg', 'IMG_1094.jpg', 'IMG_712.jpg', 'IMG_706.jpg', 'IMG_1080.jpg', 'IMG_262.jpg', 'IMG_1241.jpg', 'IMG_1240.jpg', 'IMG_1532.jpg', 'IMG_2007.jpg', 'IMG_1268.jpg', 'IMG_263.jpg', 'IMG_1283.jpg', 'IMG_505.jpg', 'IMG_539.jpg', 'IMG_1042.jpg', 'IMG_1650.jpg', 'IMG_667.jpg', 'IMG_897.jpg', 'IMG_129.jpg', 'IMG_1334.jpg', 'IMG_1452.jpg', 'IMG_1308.jpg', 'IMG_307.jpg', 'IMG_1481.jpg', 'IMG_461.jpg', 'IMG_1495.jpg', 'IMG_1442.jpg', 'IMG_1330.jpg', 'IMG_1318.jpg', 'IMG_105.jpg', 'IMG_887.jpg', 'IMG_893.jpg', 'IMG_878.jpg', 'IMG_844.jpg', 'IMG_717.jpg', 'IMG_703.jpg', 'IMG_1907.jpg', 'IMG_1734.jpg', 'IMG_1052.jpg', 'IMG_1046.jpg', 'IMG_273.jpg', '.DS_Store', 'IMG_515.jpg', 'IMG_501.jpg', 'IMG_267.jpg', 'IMG_1536.jpg', 'IMG_2017.jpg', 'IMG_298.jpg', 'IMG_299.jpg', 'IMG_1251.jpg', 'IMG_6.jpg', 'IMG_272.jpg', 'IMG_1709.jpg', 'IMG_919.jpg', 'IMG_1912.jpg', 'IMG_1084.jpg', 'IMG_702.jpg', 'IMG_1090.jpg', 'IMG_845.jpg', 'IMG_1133.jpg', 'IMG_1899.jpg', 'IMG_1127.jpg', 'IMG_1872.jpg', 'IMG_104.jpg', 'IMG_1494.jpg', 'IMG_312.jpg', 'IMG_1480.jpg', 'IMG_338.jpg', 'IMG_674.jpg', 'IMG_106.jpg', 'IMG_660.jpg', 'IMG_1858.jpg', 'IMG_884.jpg', 'IMG_1657.jpg', 'IMG_1643.jpg', 'IMG_1119.jpg', 'IMG_1723.jpg', 'IMG_264.jpg', 'IMG_270.jpg', 'IMG_1509.jpg', 'IMG_2000.jpg', 'IMG_1520.jpg', 'IMG_517.jpg', 'IMG_1285.jpg', 'IMG_932.jpg', 'IMG_1050.jpg', 'IMG_852.jpg', 'IMG_1865.jpg', 'IMG_1871.jpg', 'IMG_113.jpg', 'IMG_1468.jpg', 'IMG_1483.jpg', 'IMG_410.jpg', 'IMG_98.jpg', 'IMG_404.jpg', 'IMG_1382.jpg', 'IMG_67.jpg', 'IMG_1802.jpg', 'IMG_1194.jpg', 'IMG_174.jpg', 'IMG_606.jpg', 'IMG_835.jpg', 'IMG_1143.jpg', 'IMG_1962.jpg', 'IMG_982.jpg', 'IMG_1023.jpg', 'IMG_1037.jpg', 'IMG_1989.jpg', 'IMG_558.jpg', 'IMG_1209.jpg', 'IMG_1235.jpg', 'IMG_1552.jpg', 'IMG_1546.jpg', 'IMG_1591.jpg', 'IMG_217.jpg', 'IMG_1744.jpg', 'IMG_1977.jpg', 'IMG_1618.jpg', 'IMG_607.jpg', 'IMG_175.jpg', 'IMG_1803.jpg', 'IMG_1354.jpg', 'IMG_66.jpg', 'IMG_72.jpg', 'IMG_405.jpg', 'IMG_1397.jpg', 'IMG_377.jpg', 'IMG_1342.jpg', 'IMG_58.jpg', 'IMG_163.jpg', 'IMG_1183.jpg', 'IMG_1197.jpg', 'IMG_1975.jpg', 'IMG_1961.jpg', 'IMG_1949.jpg', 'IMG_956.jpg', 'IMG_942.jpg', 'IMG_1593.jpg', 'IMG_567.jpg', 'IMG_598.jpg', 'IMG_1550.jpg', 'IMG_1544.jpg', 'IMG_1551.jpg', 'IMG_214.jpg', 'IMG_572.jpg', 'IMG_228.jpg', 'IMG_1747.jpg', 'IMG_1035.jpg', 'IMG_943.jpg', 'IMG_764.jpg', 'IMG_1155.jpg', 'IMG_1633.jpg', 'IMG_837.jpg', 'IMG_823.jpg', 'IMG_1196.jpg', 'IMG_1814.jpg', 'IMG_638.jpg', 'IMG_59.jpg', 'IMG_1419.jpg', 'IMG_1384.jpg', 'IMG_1435.jpg', 'IMG_61.jpg', 'IMG_1623.jpg', 'IMG_1179.jpg', 'IMG_1780.jpg', 'IMG_1031.jpg', 'IMG_1541.jpg', 'IMG_588.jpg', 'IMG_1232.jpg', 'IMG_563.jpg', 'IMG_952.jpg', 'IMG_1756.jpg', 'IMG_1971.jpg', 'IMG_775.jpg', 'IMG_826.jpg', 'IMG_1622.jpg', 'IMG_1811.jpg', 'IMG_1187.jpg', 'IMG_167.jpg', 'IMG_601.jpg', 'IMG_74.jpg', 'IMG_48.jpg', 'IMG_1420.jpg', 'IMG_1346.jpg', 'IMG_371.jpg', 'IMG_89.jpg', 'IMG_367.jpg', 'IMG_1436.jpg', 'IMG_1344.jpg', 'IMG_1422.jpg', 'IMG_603.jpg', 'IMG_1813.jpg', 'IMG_159.jpg', 'IMG_1620.jpg', 'IMG_944.jpg', 'IMG_1768.jpg', 'IMG_950.jpg', 'IMG_561.jpg', 'IMG_1219.jpg', 'IMG_1557.jpg', 'IMG_1543.jpg', 'IMG_206.jpg', 'IMG_789.jpg', 'IMG_1741.jpg', 'IMG_986.jpg', 'IMG_1966.jpg', 'IMG_1609.jpg', 'IMG_1147.jpg', 'IMG_602.jpg', 'IMG_616.jpg', 'IMG_1437.jpg', 'IMG_400.jpg', 'IMG_414.jpg', 'IMG_1406.jpg', 'IMG_1837.jpg', 'IMG_169.jpg', 'IMG_828.jpg', 'IMG_1604.jpg', 'IMG_1764.jpg', 'IMG_223.jpg', 'IMG_551.jpg', 'IMG_237.jpg', 'IMG_2053.jpg', 'IMG_2047.jpg', 'IMG_1228.jpg', 'IMG_592.jpg', 'IMG_1214.jpg', 'IMG_1229.jpg', 'IMG_2052.jpg', 'IMG_550.jpg', 'IMG_791.jpg', 'IMG_1771.jpg', 'IMG_785.jpg', 'IMG_752.jpg', 'IMG_1163.jpg', 'IMG_183.jpg', 'IMG_815.jpg', 'IMG_1836.jpg', 'IMG_1349.jpg', 'IMG_53.jpg', 'IMG_92.jpg', 'IMG_51.jpg', 'IMG_1363.jpg', 'IMG_383.jpg', 'IMG_397.jpg', 'IMG_1377.jpg', 'IMG_1820.jpg', 'IMG_1808.jpg', 'IMG_803.jpg', 'IMG_817.jpg', 'IMG_195.jpg', 'IMG_1798.jpg', 'IMG_744.jpg', 'IMG_1001.jpg', 'IMG_234.jpg', 'IMG_546.jpg', 'IMG_220.jpg', 'IMG_1217.jpg', 'IMG_1202.jpg', 'IMG_1982.jpg', 'IMG_1799.jpg', 'IMG_1941.jpg', 'IMG_1174.jpg', 'IMG_1148.jpg', 'IMG_802.jpg', 'IMG_355.jpg', 'IMG_423.jpg', 'IMG_379.jpg', 'IMG_68.jpg', 'IMG_621.jpg', 'IMG_1819.jpg', 'IMG_635.jpg', 'IMG_153.jpg', 'IMG_1831.jpg', 'IMG_1158.jpg', 'IMG_755.jpg', 'IMG_1979.jpg', 'IMG_999.jpg', 'IMG_543.jpg', 'IMG_1212.jpg', 'IMG_1206.jpg', 'IMG_1560.jpg', 'IMG_2041.jpg', 'IMG_581.jpg', 'IMG_1575.jpg', 'IMG_1213.jpg', 'IMG_542.jpg', 'IMG_967.jpg', 'IMG_1011.jpg', 'IMG_797.jpg', 'IMG_1978.jpg', 'IMG_813.jpg', 'IMG_1603.jpg', 'IMG_608.jpg', 'IMG_1429.jpg', 'IMG_69.jpg', 'IMG_1415.jpg', 'IMG_96.jpg', 'IMG_420.jpg', 'IMG_94.jpg', 'IMG_385.jpg', 'IMG_57.jpg', 'IMG_1359.jpg', 'IMG_144.jpg', 'IMG_1826.jpg', 'IMG_805.jpg', 'IMG_756.jpg', 'IMG_1007.jpg', 'IMG_959.jpg', 'IMG_1761.jpg', 'IMG_965.jpg', 'IMG_1991.jpg', 'IMG_583.jpg', 'IMG_1211.jpg', 'IMG_597.jpg', 'IMG_2042.jpg', 'IMG_1238.jpg', 'IMG_582.jpg', 'IMG_1589.jpg', 'IMG_233.jpg', 'IMG_227.jpg', 'IMG_794.jpg', 'IMG_1628.jpg', 'IMG_1600.jpg', 'IMG_1172.jpg', 'IMG_1199.jpg', 'IMG_151.jpg', 'IMG_384.jpg', 'IMG_1370.jpg', 'IMG_421.jpg', 'IMG_308.jpg', 'IMG_31.jpg', 'IMG_1459.jpg', 'IMG_485.jpg', 'IMG_19.jpg', 'IMG_1854.jpg', 'IMG_136.jpg', 'IMG_877.jpg', 'IMG_1883.jpg', 'IMG_1101.jpg', 'IMG_1908.jpg', 'IMG_730.jpg', 'IMG_917.jpg', 'IMG_1713.jpg', 'IMG_240.jpg', 'IMG_1511.jpg', 'IMG_2018.jpg', 'IMG_1276.jpg', 'IMG_296.jpg', 'IMG_2025.jpg', 'IMG_2031.jpg', 'IMG_1538.jpg', 'IMG_1289.jpg', 'IMG_269.jpg', 'IMG_1074.jpg', 'IMG_1706.jpg', 'IMG_1909.jpg', 'IMG_1935.jpg', 'IMG_862.jpg', 'IMG_1882.jpg', 'IMG_876.jpg', 'IMG_645.jpg', 'IMG_123.jpg', 'IMG_137.jpg', 'IMG_1699.jpg', 'IMG_679.jpg', 'IMG_24.jpg', 'IMG_30.jpg', 'IMG_321.jpg', 'IMG_451.jpg', 'IMG_32.jpg', 'IMG_1466.jpg', 'IMG_121.jpg', 'IMG_647.jpg', 'IMG_135.jpg', 'IMG_1664.jpg', 'IMG_848.jpg', 'IMG_1923.jpg', 'IMG_727.jpg', 'IMG_914.jpg', 'IMG_1248.jpg', 'IMG_1512.jpg', 'IMG_280.jpg', 'IMG_1506.jpg', 'IMG_295.jpg', 'IMG_524.jpg', 'IMG_1936.jpg', 'IMG_691.jpg', 'IMG_1117.jpg', 'IMG_1659.jpg', 'IMG_134.jpg', 'IMG_1842.jpg', 'IMG_487.jpg', 'IMG_322.jpg', 'IMG_478.jpg', 'IMG_497.jpg', 'IMG_1339.jpg', 'IMG_37.jpg', 'IMG_1852.jpg', 'IMG_681.jpg', 'IMG_1661.jpg', 'IMG_695.jpg', 'IMG_1885.jpg', 'IMG_722.jpg', 'IMG_1073.jpg', 'IMG_252.jpg', 'IMG_508.jpg', 'IMG_285.jpg', 'IMG_2023.jpg', 'IMG_1502.jpg', 'IMG_1264.jpg', 'IMG_1270.jpg', 'IMG_1516.jpg', 'IMG_535.jpg', 'IMG_904.jpg', 'IMG_1728.jpg', 'IMG_938.jpg', 'IMG_1072.jpg', 'IMG_1099.jpg', 'IMG_1890.jpg', 'IMG_864.jpg', 'IMG_1674.jpg', 'IMG_119.jpg', 'IMG_1462.jpg', 'IMG_455.jpg', 'IMG_1474.jpg', 'IMG_480.jpg', 'IMG_1306.jpg', 'IMG_34.jpg', 'IMG_641.jpg', 'IMG_1851.jpg', 'IMG_682.jpg', 'IMG_1892.jpg', 'IMG_866.jpg', 'IMG_1919.jpg', 'IMG_709.jpg', 'IMG_1058.jpg', 'IMG_912.jpg', 'IMG_245.jpg', 'IMG_251.jpg', 'IMG_279.jpg', 'IMG_1500.jpg', 'IMG_2021.jpg', 'IMG_1529.jpg', 'IMG_2020.jpg', 'IMG_293.jpg', 'IMG_1501.jpg', 'IMG_278.jpg', 'IMG_1298.jpg', 'IMG_913.jpg', 'IMG_907.jpg', 'IMG_1059.jpg', 'IMG_1071.jpg', 'IMG_1703.jpg', 'IMG_708.jpg', 'IMG_1918.jpg', 'IMG_867.jpg', 'IMG_873.jpg', 'IMG_1878.jpg', 'IMG_35.jpg', 'IMG_1449.jpg', 'IMG_1307.jpg', 'IMG_1475.jpg']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "bs = 8\n",
    "img_w = 256\n",
    "img_h = 256\n",
    "validation_split = 0.1\n",
    "\n",
    "classes = [\n",
    "    'owl',              # 0\n",
    "    'galaxy',           # 1\n",
    "    'lightning',        # 2\n",
    "    'wine-bottle',      # 3\n",
    "    't-shirt',          # 4\n",
    "    'waterfall',        # 5\n",
    "    'sword',            # 6\n",
    "    'school-bus',       # 7\n",
    "    'calculator',       # 8\n",
    "    'sheet-music',      # 9\n",
    "    'airplanes',        # 10\n",
    "    'lightbulb',        # 11\n",
    "    'skyscraper',       # 12\n",
    "    'mountain-bike',    # 13\n",
    "    'fireworks',        # 14\n",
    "    'computer-monitor', # 15\n",
    "    'bear',             # 16\n",
    "    'grand-piano',      # 17\n",
    "    'kangaroo',         # 18\n",
    "    'laptop'            # 19\n",
    "]\n",
    "\n",
    "# LOAD TRAINING AND VALIDATION SETS\n",
    "\n",
    "data_gen = ImageDataGenerator(rotation_range = 10,\n",
    "                              width_shift_range = 10,\n",
    "                              height_shift_range = 10,\n",
    "                              zoom_range = 0.3,\n",
    "                              horizontal_flip = True,\n",
    "                              vertical_flip = True,\n",
    "                              rescale = 1./255, \n",
    "                              validation_split = validation_split)\n",
    "\n",
    "training_dir = os.path.join(cwd, \"Classification_Dataset\", \"training\")\n",
    "\n",
    "print(\"Training\")\n",
    "\n",
    "training_generator = data_gen.flow_from_directory(training_dir,\n",
    "                                        batch_size = bs,\n",
    "                                        classes = classes,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        shuffle = True,\n",
    "                                        seed = SEED,\n",
    "                                        subset = 'training')\n",
    "\n",
    "print(\"\\nValidation\")\n",
    "\n",
    "validation_generator = data_gen.flow_from_directory(training_dir,\n",
    "                                        batch_size = bs,\n",
    "                                        classes = classes,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        shuffle = True,\n",
    "                                        seed = SEED,\n",
    "                                        subset = 'validation')\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_generator(lambda: training_generator,\n",
    "                                              output_types = (tf.float32, tf.float32),\n",
    "                                              output_shapes = ([None, img_w, img_h, 3], [None, len(classes)]))\n",
    "validation_dataset = tf.data.Dataset.from_generator(lambda: validation_generator,\n",
    "                                              output_types = (tf.float32, tf.float32),\n",
    "                                              output_shapes = ([None, img_w, img_h, 3], [None, len(classes)]))\n",
    "\n",
    "training_dataset = training_dataset.repeat()\n",
    "validation_dataset = validation_dataset.repeat()\n",
    "\n",
    "# WRITE FILENAMES TO JSON FILE\n",
    "\n",
    "filenames = {\n",
    "    \"training\" : {},\n",
    "    \"validation\" : {}\n",
    "}\n",
    "\n",
    "for c in classes:\n",
    "    filenames[\"training\"][c] = []\n",
    "    filenames[\"validation\"][c] = []\n",
    "    \n",
    "    for fn in training_generator.filenames:\n",
    "        if c in fn:\n",
    "            filenames[\"training\"][c].append(fn.replace(c + \"/\", \"\"))\n",
    "    \n",
    "    for fn in validation_generator.filenames:\n",
    "        if c in fn:\n",
    "            filenames[\"validation\"][c].append(fn.replace(c + \"/\", \"\"))\n",
    "\n",
    "with open('dataset_split.json', 'w') as file:\n",
    "    json.dump(filenames, file, indent=4)\n",
    "    \n",
    "\n",
    "# LOAD TEST SET filenames\n",
    "\n",
    "print(\"\\nTo predict\")\n",
    "\n",
    "test_dir = os.path.join(cwd, \"Classification_Dataset\", \"test\")\n",
    "test_filenames = next(os.walk(test_dir))[2]\n",
    "\n",
    "print(\"Found \" + str(len(test_filenames)) + \" images.\")\n",
    "\n",
    "print(test_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building, fitting and predicting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, conv_depth, start_num_filters, kernel_size, pool_size, fc_units, num_classes):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(conv_depth):\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(filters = start_num_filters,\n",
    "                             kernel_size = kernel_size,\n",
    "                             strides = (1, 1),\n",
    "                             padding = 'same',\n",
    "                             activation = 'relu',\n",
    "                             input_shape = input_shape))\n",
    "        else:\n",
    "            model.add(Conv2D(filters = start_num_filters,\n",
    "                             kernel_size = kernel_size,\n",
    "                             strides = (1, 1),\n",
    "                             padding = 'same',\n",
    "                             activation = 'relu'))\n",
    "            \n",
    "        model.add(MaxPool2D(pool_size = pool_size))\n",
    "        start_num_filters *= 2\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for i in range(len(fc_units)):\n",
    "        model.add(Dense(units = fc_units[i], activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(units = num_classes, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, model_name = datetime.now().strftime('%b%d_%H-%M-%S')):\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # General experiments folder\n",
    "    exps_dir = os.path.join(cwd, 'classification_experiments')\n",
    "    if not os.path.exists(exps_dir):\n",
    "        os.makedirs(exps_dir)\n",
    "    \n",
    "    now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    \n",
    "    # This experiment folder\n",
    "    exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "    if not os.path.exists(exp_dir):\n",
    "        os.makedirs(exp_dir)\n",
    "    \n",
    "    # Checpoints folder\n",
    "    ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    \n",
    "    # Checkpoints callback, best one will be the last saved\n",
    "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                       save_weights_only=True,\n",
    "                                                       save_best_only=True, \n",
    "                                                       monitor='val_loss', \n",
    "                                                       mode = 'min')\n",
    "    \n",
    "    # Tensorboard folder\n",
    "    tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "    if not os.path.exists(tb_dir):\n",
    "        os.makedirs(tb_dir)\n",
    "    \n",
    "    # Tensorboard callback\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                                 profile_batch=0,\n",
    "                                                 histogram_freq=1)  # if 1 shows weights histograms\n",
    "    \n",
    "    # Early stopping callback\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                   patience=10,\n",
    "                                                   mode = 'min')\n",
    "    \n",
    "    callbacks= [ckpt_callback, tb_callback, es_callback]\n",
    "    \n",
    "    model.fit(x=training_dataset,\n",
    "              epochs=100,\n",
    "              steps_per_epoch=len(training_generator),\n",
    "              validation_data=validation_dataset,\n",
    "              validation_steps=len(validation_generator), \n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Load best model (last one saved)\n",
    "    latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "    print(\"Latest model: \" + latest)\n",
    "    model.load_weights(os.path.join(ckpt_dir, latest))\n",
    "    \n",
    "    return (model, exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, exp_dir):\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for image_name in test_filenames:\n",
    "        img = Image.open(os.path.join(test_dir,image_name)).convert('RGB').resize((img_w, img_h))\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, 0)\n",
    "        img_array = np.divide(img_array,255)\n",
    "        tensor = tf.convert_to_tensor(img_array, dtype = tf.float32)\n",
    "        \n",
    "        prediction = np.argmax(model.predict(tensor))\n",
    "        results[image_name] = prediction\n",
    "\n",
    "    create_csv(results = results, results_dir=exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 256, 256, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 128, 128, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 128, 128, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 4,303,460\n",
      "Trainable params: 4,303,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 176 steps, validate for 19 steps\n",
      "Epoch 1/100\n",
      "176/176 [==============================] - 76s 431ms/step - loss: 2.9120 - accuracy: 0.0839 - val_loss: 2.7472 - val_accuracy: 0.1014\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 79s 446ms/step - loss: 2.6738 - accuracy: 0.1622 - val_loss: 2.5157 - val_accuracy: 0.2095\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 79s 450ms/step - loss: 2.4528 - accuracy: 0.2361 - val_loss: 2.4221 - val_accuracy: 0.2027\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 79s 451ms/step - loss: 2.2630 - accuracy: 0.2945 - val_loss: 2.2945 - val_accuracy: 0.2770\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 79s 449ms/step - loss: 2.1248 - accuracy: 0.3599 - val_loss: 2.3344 - val_accuracy: 0.2973\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 79s 447ms/step - loss: 1.9854 - accuracy: 0.3876 - val_loss: 1.9838 - val_accuracy: 0.4122\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 78s 444ms/step - loss: 1.8548 - accuracy: 0.4189 - val_loss: 1.9717 - val_accuracy: 0.3919\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 78s 443ms/step - loss: 1.7233 - accuracy: 0.4780 - val_loss: 1.8795 - val_accuracy: 0.3716\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 79s 447ms/step - loss: 1.6617 - accuracy: 0.4708 - val_loss: 1.8696 - val_accuracy: 0.3986\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 77s 439ms/step - loss: 1.5571 - accuracy: 0.5043 - val_loss: 1.7260 - val_accuracy: 0.4595\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 77s 436ms/step - loss: 1.4944 - accuracy: 0.5220 - val_loss: 1.8591 - val_accuracy: 0.4392\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 77s 439ms/step - loss: 1.3892 - accuracy: 0.5555 - val_loss: 1.6232 - val_accuracy: 0.4797\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 77s 435ms/step - loss: 1.3230 - accuracy: 0.5910 - val_loss: 1.7301 - val_accuracy: 0.4932\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 76s 431ms/step - loss: 1.2725 - accuracy: 0.6003 - val_loss: 1.6822 - val_accuracy: 0.4865\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 76s 432ms/step - loss: 1.1504 - accuracy: 0.6316 - val_loss: 1.7369 - val_accuracy: 0.5135\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 76s 433ms/step - loss: 1.1723 - accuracy: 0.6394 - val_loss: 1.4803 - val_accuracy: 0.5473\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 76s 431ms/step - loss: 1.1063 - accuracy: 0.6394 - val_loss: 1.7053 - val_accuracy: 0.5068\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 76s 432ms/step - loss: 1.0393 - accuracy: 0.6522 - val_loss: 1.8121 - val_accuracy: 0.5405\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 76s 432ms/step - loss: 0.9973 - accuracy: 0.6814 - val_loss: 1.6627 - val_accuracy: 0.5405\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 76s 432ms/step - loss: 0.8653 - accuracy: 0.7155 - val_loss: 1.7490 - val_accuracy: 0.4865\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 76s 432ms/step - loss: 0.9212 - accuracy: 0.7027 - val_loss: 1.9159 - val_accuracy: 0.5203\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - 76s 432ms/step - loss: 0.8265 - accuracy: 0.7297 - val_loss: 1.9401 - val_accuracy: 0.4865\n",
      "Epoch 23/100\n",
      "176/176 [==============================] - 76s 433ms/step - loss: 0.8196 - accuracy: 0.7496 - val_loss: 1.7308 - val_accuracy: 0.5068\n",
      "Epoch 24/100\n",
      "176/176 [==============================] - 76s 434ms/step - loss: 0.8018 - accuracy: 0.7411 - val_loss: 1.7024 - val_accuracy: 0.5405\n",
      "Epoch 25/100\n",
      "176/176 [==============================] - 76s 433ms/step - loss: 0.7684 - accuracy: 0.7667 - val_loss: 1.8235 - val_accuracy: 0.5676\n",
      "Epoch 26/100\n",
      "176/176 [==============================] - 76s 433ms/step - loss: 0.7028 - accuracy: 0.7724 - val_loss: 1.8056 - val_accuracy: 0.5473\n",
      "Latest model: /Users/andrea/Documents/Code/GitHub/A2NDL-HW/01 - Image Classification/classification_experiments/CNN_1_Nov18_21-11-54/ckpts/cp_16.ckpt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CheckpointLoadStatus' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-9dacd36c940e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-d50cb82feca3>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, exp_dir)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CheckpointLoadStatus' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "model_name = \"CNN_1\"\n",
    "conv_depth = 5\n",
    "start_num_filters = 8\n",
    "kernel_size = (3, 3)\n",
    "pool_size = (2, 2)\n",
    "fc_units = [512]\n",
    "num_classes = len(classes)\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model = build_model(input_shape = (img_w, img_h, 3),\n",
    "                   conv_depth = conv_depth,\n",
    "                   start_num_filters = start_num_filters,\n",
    "                   kernel_size = kernel_size,\n",
    "                   pool_size = pool_size,\n",
    "                   fc_units = fc_units,\n",
    "                   num_classes = num_classes)\n",
    "\n",
    "model.compile(optimizer = optimizer,\n",
    "             loss = loss,\n",
    "             metrics = metrics)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "(model, exp_dir) = fit_model(model = model, model_name = model_name)\n",
    "\n",
    "predict(model = model, exp_dir = exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 [datascience]",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
