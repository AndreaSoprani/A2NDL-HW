{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering\n",
    "\n",
    "In this notebook I used two different model architectures to solve the visual question answering problem.\n",
    "\n",
    "The images are processed with the VGG19 (stripped of the softmax layer). The result is a vector of 4096 values.\n",
    "\n",
    "The questions are processed with GloVe, the result is a vector of 300 values per each word, I used a fixed length of 50 tokens per question.\n",
    "\n",
    "The first model uses LSTM units to model the questions and a FC takes the image vector and the LSTM output as an input.\n",
    "\n",
    "The second model uses two FC networks: the first one takes the flattened question matrix ((50, 300) -> 15000) and reduces its size to a more manageable number of values. The second one takes this resulting vector and concatenates it to the image representation to make predictions.\n",
    "\n",
    "Due to personal time constraints I wasn't able to tune appropriately the hyperparameters and I've also been forced to keep batch sizes and steps per epoch low. Furthermore I was not able to try other approaches.\n",
    "\n",
    "The results are similar for the two different architectures, the maximum test accuracy was 0.25 for both architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import spacy\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 224\n",
    "width = 224\n",
    "shape = (height, width, 3)\n",
    "\n",
    "bs = 16 # Batch size\n",
    "\n",
    "question_max_length = 50\n",
    "\n",
    "num_classes = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"dataset_vqa/train_data.json\"\n",
    "train_images_dir = \"dataset_vqa/train/\"\n",
    "\n",
    "test_file = \"dataset_vqa/test_data.json\"\n",
    "test_images_dir = \"dataset_vqa/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.1\n",
    "\n",
    "with open(train_file) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "indices = list(range(len(data[\"questions\"])))\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_length = int(len(data[\"questions\"])*(1-validation_split))\n",
    "\n",
    "train_indices = indices[:train_length]\n",
    "validation_indices = indices[train_length:]\n",
    "\n",
    "steps_per_epoch = int(len(train_indices) / bs)\n",
    "validation_steps = int(len(validation_indices) / bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 139,570,240\n",
      "Trainable params: 0\n",
      "Non-trainable params: 139,570,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Image model for extracting a vector of features\n",
    "# VGG19\n",
    "\n",
    "image_model = tf.keras.applications.VGG19(input_shape = shape,\n",
    "                                          include_top = True,\n",
    "                                          weights = 'imagenet')\n",
    "image_model = Model(image_model.input, image_model.layers[-2].output)\n",
    "image_model.trainable = False\n",
    "image_model.summary()\n",
    "\n",
    "image_features_size = image_model.layers[-1].output_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_features(image_model, filenames):\n",
    "    \"\"\"\n",
    "    Returns the vectors of features of images given an image model and the filenames list.\n",
    "    \n",
    "    Parameters:\n",
    "        image_model (keras model): pre-trained keras model that given an image outputs a vector of features (e.g. VGG16 without last layer).\n",
    "        filenames ([str]): list of strings representing the images from this folder.\n",
    "        \n",
    "    Returns:\n",
    "        a list of feature vectors (one per image).\n",
    "    \"\"\"\n",
    "    \n",
    "    images = []\n",
    "    for fn in filenames:\n",
    "        img = Image.open(fn)\n",
    "            \n",
    "        img = img.resize((height, width))\n",
    "        img = np.array(img)[:,:,:3]\n",
    "        images.append(img)\n",
    "    \n",
    "    images = tf.convert_to_tensor(images)\n",
    "    return tf.convert_to_tensor(image_model.predict(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding model\n",
    "# GloVe\n",
    "\n",
    "word_model = spacy.load(\"en_core_web_lg\")\n",
    "word_features_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_features(word_model, questions):\n",
    "    \"\"\"\n",
    "    Returns the vectors of features of questions.\n",
    "    \n",
    "    Parameters:\n",
    "        word_model: word embedding model (e.g. \"en_core_web_lg\").\n",
    "        questions ([str]): list of questions as strings.\n",
    "        \n",
    "    Returns:\n",
    "        a list of Glove vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    questions_features = np.zeros((len(questions), question_max_length, word_features_size))\n",
    "    \n",
    "    for q in range(len(questions)):\n",
    "        tokens = word_model(questions[q])\n",
    "        for i in range(len(tokens)):\n",
    "            questions_features[q, i, :] = tokens[i].vector\n",
    "        \n",
    "    return tf.convert_to_tensor(questions_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dict = {\n",
    "    '0': 0,\n",
    "    '1': 1,\n",
    "    '10': 2,\n",
    "    '2': 3,\n",
    "    '3': 4,\n",
    "    '4': 5,\n",
    "    '5': 6,\n",
    "    '6': 7,\n",
    "    '7': 8,\n",
    "    '8': 9,\n",
    "    '9': 10,\n",
    "    'no': 11,\n",
    "    'yes': 12\n",
    "}\n",
    "\n",
    "def encode(y):\n",
    "    y = encode_dict[y]\n",
    "    return tf.one_hot(y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(indices, batch_size = bs, flatten_questions = False):\n",
    "    \"\"\"\n",
    "    A dataset generator\n",
    "    Parameters:\n",
    "        indices ([int]): list of indices of the set as found in the train_data.json file.\n",
    "        batch_size (int): batch size.\n",
    "        flatten_questions (bool): optionally the question matrix of each question can be flattened into a single vector (default False). \n",
    "    \n",
    "    Returns:\n",
    "        the dataset generator.\n",
    "    \"\"\"\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        batch_indices = np.random.choice(a = indices, size = batch_size)\n",
    "        \n",
    "        with open(train_file) as f:\n",
    "            data = json.load(f)\n",
    "            data = data[\"questions\"]\n",
    "        \n",
    "        questions = []\n",
    "        images_filenames = []\n",
    "        answers = []\n",
    "        \n",
    "        for i in batch_indices:\n",
    "            q = data[i]\n",
    "            questions.append(q[\"question\"])\n",
    "            images_filenames.append(train_images_dir + q[\"image_filename\"])\n",
    "            answers.append(encode(q[\"answer\"]))\n",
    "        \n",
    "        images_features = get_images_features(image_model, images_filenames)\n",
    "        questions_features = get_questions_features(word_model, questions)\n",
    "        \n",
    "        if flatten_questions:\n",
    "            questions_features = tf.reshape(questions_features, [len(batch_indices), question_max_length*word_features_size])\n",
    "        \n",
    "        batch_x = [questions_features, images_features]\n",
    "        batch_y = np.array(answers)\n",
    "        \n",
    "        yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_indices, bs)\n",
    "validation_generator = generator(validation_indices, bs)\n",
    "\n",
    "train_generator_alt = generator(train_indices, bs, flatten_questions = True)\n",
    "validation_generator_alt = generator(validation_indices, bs, flatten_questions = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(LSTM_units, FC_units, FC_dropout=None):\n",
    "    \"\"\"\n",
    "    Returns a Keras model given some parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        LSTM_units ([int]): number of LSTM units in each layer.\n",
    "        FC_units ([int]): number of units in each FC layer.\n",
    "        FC_dropout (float): dropout for the FC layers (default None)\n",
    "        \n",
    "    Returns:\n",
    "        a Keras model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Image\n",
    "    in_image = Input((image_features_size,))\n",
    "    \n",
    "    in_language = Input((question_max_length, word_features_size))\n",
    "    model_language = in_language\n",
    "    for i in range(len(LSTM_units)):\n",
    "        if i < len(LSTM_units) - 1:\n",
    "            model_language = LSTM(LSTM_units[i], return_sequences = True)(model_language)\n",
    "        else:\n",
    "            model_language = LSTM(LSTM_units[i], return_sequences = False)(model_language)\n",
    "    \n",
    "    model = Concatenate()([model_language, in_image])\n",
    "    \n",
    "    for i in range(len(FC_units)):\n",
    "        model = Dense(FC_units[i], activation = 'relu')(model)\n",
    "        if FC_dropout:\n",
    "            model = Dropout(FC_dropout)(model)\n",
    "    \n",
    "    model = Dense(num_classes, activation = 'softmax')(model)\n",
    "    \n",
    "    model = Model([in_language, in_image], model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_alt(FC1_units, FC2_units,\n",
    "                  FC1_dropout = None, FC1_activation = 'relu', \n",
    "                  FC2_dropout = None, FC2_activation = 'relu'):\n",
    "    \"\"\"\n",
    "    Returns a Keras model given some parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        FC1_units ([int]): number of units in each FC_1 layer (question processing).\n",
    "        FC2_units ([int]): number of units in each FC_2 layer (question + image processing).\n",
    "        FC1_dropout (float): dropout for the FC_1 layers (default None).\n",
    "        FC1_activation (str): name of the activation function used in the FC_1 layers.\n",
    "        FC2_dropout (float): dropout for the FC_2 layers (default None).\n",
    "        FC2_activation (str): name of the activation function used in the FC_2 layers. \n",
    "        \n",
    "    Returns:\n",
    "        a Keras model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    in_image = Input((image_features_size,))\n",
    "    in_language = Input((question_max_length * word_features_size))\n",
    "    \n",
    "    model_language = in_language\n",
    "    for i in range(len(FC1_units)):\n",
    "        model_language = Dense(FC1_units[i], activation = FC1_activation)(model_language)\n",
    "        if FC1_dropout:\n",
    "            model_language = Dropout(FC1_dropout)(model_language)\n",
    "            \n",
    "    model = Concatenate()([model_language, in_image])\n",
    "    \n",
    "    for i in range(len(FC2_units)):\n",
    "        model = Dense(FC2_units[i], activation = FC2_activation)(model)\n",
    "        if FC2_dropout:\n",
    "            model = Dropout(FC2_dropout)(model)\n",
    "            \n",
    "    model = Dense(num_classes, activation = 'softmax')(model)\n",
    "    \n",
    "    model = Model([in_language, in_image], model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, model_name = datetime.now().strftime('%b%d_%H-%M-%S'), train_gen = train_generator, validation_gen = validation_generator):\n",
    "    \"\"\"\n",
    "    Function used to fit the model (and save the checkpoints).\n",
    "    It saves all the checkpoints that increased the performance and returns the best one.\n",
    "    The performance evaluated is the loss.\n",
    "    Early stopping is used with 10 epochs of patience.\n",
    "    It also uses a tensorboard callback for visualization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: keras model\n",
    "        model to fit.\n",
    "    model_name: string, optional\n",
    "        name of the model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    keras model: the best model.\n",
    "    string: the directory of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # General experiments folder\n",
    "    exps_dir = os.path.join(cwd, 'vqa_experiments')\n",
    "    if not os.path.exists(exps_dir):\n",
    "        os.makedirs(exps_dir)\n",
    "    \n",
    "    now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    \n",
    "    # This experiment folder\n",
    "    exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "    if not os.path.exists(exp_dir):\n",
    "        os.makedirs(exp_dir)\n",
    "    \n",
    "    # Checpoints folder\n",
    "    ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    \n",
    "    # Tensorboard folder\n",
    "    tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "    if not os.path.exists(tb_dir):\n",
    "        os.makedirs(tb_dir)\n",
    "    \n",
    "    # Checkpoints callback, best one will be the last saved\n",
    "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                       save_weights_only=True,\n",
    "                                                       save_best_only=True)\n",
    "    \n",
    "    # Tensorboard callback\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                                 profile_batch=0,\n",
    "                                                 histogram_freq=1)  # if 1 shows weights histograms\n",
    "    \n",
    "    # Early stopping callback\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    callbacks= [ckpt_callback, tb_callback, es_callback]\n",
    "    \n",
    "    model.fit_generator(generator=train_gen,\n",
    "                        epochs=50,\n",
    "                        steps_per_epoch=100,\n",
    "                        validation_data=validation_gen,\n",
    "                        validation_steps=10,\n",
    "                        callbacks=callbacks,\n",
    "                        use_multiprocessing=False,\n",
    "                        workers=1)\n",
    "    \n",
    "    # Load best model (last one saved)\n",
    "    latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "    print(\"Latest model: \" + latest)\n",
    "    model.load_weights(os.path.join(ckpt_dir, latest))\n",
    "    \n",
    "    return (model, exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(results, results_dir='./'):\n",
    "    \"\"\"\n",
    "    Function used to write a prediction dictionary to a csv file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    results: dict\n",
    "        predictions\n",
    "    results_dir: string, optional\n",
    "        the directory\n",
    "    \"\"\"\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(str(key) + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, exp_dir, flatten_questions = False):\n",
    "    \"\"\"\n",
    "    Function used to make predictions and write them to file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: keras model\n",
    "    exp_dir: the directory where the predictions must be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"Evaluating the test set...\")\n",
    "    \n",
    "    with open(test_file) as f:\n",
    "        data = json.load(f)\n",
    "        data = data[\"questions\"]\n",
    "        \n",
    "    num_questions = len(data)\n",
    "    \n",
    "    for i in range(num_questions):\n",
    "        q = data[i]\n",
    "        question = [q[\"question\"]]\n",
    "        image_filename = [test_images_dir + q[\"image_filename\"]]\n",
    "        \n",
    "        question_features = get_questions_features(word_model, question)\n",
    "        if flatten_questions:\n",
    "            question_features = tf.reshape(question_features, [1, question_max_length*word_features_size])\n",
    "            \n",
    "        image_features = get_images_features(image_model, image_filename)\n",
    "        \n",
    "        prediction = model.predict([question_features, image_features])\n",
    "        prediction = np.argmax(prediction)\n",
    "        \n",
    "        question_id = q[\"question_id\"]\n",
    "        \n",
    "        progress = (question_id / num_questions) * 100\n",
    "        \n",
    "        if progress % 10 == 0:\n",
    "            print(\"Progress: \" + str(int(progress)) + \"%\")\n",
    "        \n",
    "        results[question_id] = prediction\n",
    "    \n",
    "    print(\"DONE\")\n",
    "    \n",
    "    create_csv(results = results, results_dir=exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(LSTM_units, FC_units, FC_dropout = None):\n",
    "    \"\"\"\n",
    "    Function used to group all the operations needed to create a LSTM+FC model, compile it, fit it and make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name = \"LSTM_u=\"\n",
    "    for u in LSTM_units:\n",
    "        model_name += str(u) + \"_\"\n",
    "    model_name += \"FC_u=\"\n",
    "    for u in FC_units:\n",
    "        model_name += str(u) + \"_\"\n",
    "    if FC_dropout:\n",
    "        model_name += \"FC_do\" + str(FC_dropout).replace(\"0.\", \"p\")\n",
    "        \n",
    "    model = get_model(LSTM_units = LSTM_units, \n",
    "                      FC_units = FC_units, \n",
    "                      FC_dropout = FC_dropout)\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    lr = 1e-2\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    metrics = ['accuracy']\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    model, m_dir = fit_model(model, model_name)\n",
    "    \n",
    "    predict(model, m_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_alt(FC1_units, FC2_units, \n",
    "                  FC1_dropout = None, FC1_activation = 'relu', \n",
    "                  FC2_dropout = None, FC2_activation = 'relu'):\n",
    "    \"\"\"\n",
    "    Function used to group all the operations needed to create a FC+FC model, compile it, fit it and make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name = \"FC1_u=\"\n",
    "    for u in FC1_units:\n",
    "        model_name += str(u) + \"_\"\n",
    "    if FC1_dropout:\n",
    "        model_name += \"do=\" + str(FC1_dropout).replace(\"0.\", \"p\") + \"_\"\n",
    "    #model_name += \"_a=\" + FC1_activation\n",
    "    model_name += \"FC2_u=\"\n",
    "    for u in FC2_units:\n",
    "        model_name += str(u) + \"_\"\n",
    "    if FC2_dropout:\n",
    "        model_name += \"_do=\" + str(FC2_dropout).replace(\"0.\", \"p\") + \"_\"\n",
    "    #model_name += \"_a=\" + FC2_activation\n",
    "    \n",
    "    model = get_model_alt(FC1_units, FC2_units,\n",
    "                          FC1_dropout, FC1_activation, \n",
    "                          FC2_dropout, FC2_activation)\n",
    "        \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    lr = 1e-2\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    metrics = ['accuracy']\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model, m_dir = fit_model(model, model_name, train_gen = train_generator_alt, validation_gen = validation_generator_alt)\n",
    "    \n",
    "    predict(model, m_dir, flatten_questions = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1\n",
    "\n",
    "The image is processed with VGG19 to obtain a vector of 4096 values.\n",
    "\n",
    "The question is turned into a vector of 50 GloVe vectors (one per word + padding).\n",
    "\n",
    "The question is then fed to some LSTM layers, the output of the last LSTM layer is concatenated with the image vector and this resulting vector is fed to some FC layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTM_depth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-28411fcc22ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m run_model(LSTM_units = LSTM_units,\n\u001b[1;32m----> 5\u001b[1;33m           FC_units = FC_units)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-c3327a8fcda8>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(LSTM_units, FC_units, FC_dropout)\u001b[0m\n\u001b[0;32m     12\u001b[0m     model = get_model(LSTM_units = LSTM_units, \n\u001b[0;32m     13\u001b[0m                       \u001b[0mFC_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFC_units\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                       FC_dropout = FC_dropout)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-7bfb73b552ba>\u001b[0m in \u001b[0;36mget_model\u001b[1;34m(LSTM_units, FC_units, FC_dropout)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmodel_language\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_language\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mLSTM_depth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mmodel_language\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM_units\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_language\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LSTM_depth' is not defined"
     ]
    }
   ],
   "source": [
    "LSTM_units = [512, 512, 512]\n",
    "FC_units = [512, 512, 512]\n",
    "\n",
    "run_model(LSTM_units = LSTM_units,\n",
    "          FC_units = FC_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 50, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 50, 512)      1665024     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 50, 512)      2099200     lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 512)          2099200     lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4608)         0           lstm_5[0][0]                     \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          2359808     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          262656      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 13)           6669        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,755,213\n",
      "Trainable params: 8,755,213\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 1477s 15s/step - loss: 12.9902 - accuracy: 0.1875 - val_loss: 12.2900 - val_accuracy: 0.2375\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 1584s 16s/step - loss: 13.4385 - accuracy: 0.1663 - val_loss: 13.2974 - val_accuracy: 0.1750\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 1599s 16s/step - loss: 13.3377 - accuracy: 0.1725 - val_loss: 13.0960 - val_accuracy: 0.1875\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 1600s 16s/step - loss: 13.2370 - accuracy: 0.1787 - val_loss: 12.6930 - val_accuracy: 0.2125\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 1611s 16s/step - loss: 13.2370 - accuracy: 0.1787 - val_loss: 12.6930 - val_accuracy: 0.2125\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 1551s 16s/step - loss: 13.5593 - accuracy: 0.1587 - val_loss: 14.3048 - val_accuracy: 0.1125\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 1571s 16s/step - loss: 12.9751 - accuracy: 0.1950 - val_loss: 12.4915 - val_accuracy: 0.2250\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 1517s 15s/step - loss: 13.4183 - accuracy: 0.1675 - val_loss: 13.9019 - val_accuracy: 0.1375\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 1532s 15s/step - loss: 13.0758 - accuracy: 0.1887 - val_loss: 13.0960 - val_accuracy: 0.1875\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 1536s 15s/step - loss: 13.4385 - accuracy: 0.1663 - val_loss: 12.6930 - val_accuracy: 0.2125\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 1541s 15s/step - loss: 13.2974 - accuracy: 0.1750 - val_loss: 13.7004 - val_accuracy: 0.1500\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\03 - Visual Question Answering\\vqa_experiments\\LSTMu=512_LSTMd=3_FCu=512_FCd=3_FCdop3_Jan13_22-00-50\\ckpts\\cp_01.ckpt\n",
      "Evaluating the test set...\n",
      "Progress: 0%\n",
      "Progress: 10%\n",
      "Progress: 20%\n",
      "Progress: 30%\n",
      "Progress: 40%\n",
      "Progress: 50%\n",
      "Progress: 60%\n",
      "Progress: 70%\n",
      "Progress: 80%\n",
      "Progress: 90%\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "LSTM_units = [512, 512, 512]\n",
    "FC_units = [512, 512, 512]\n",
    "FC_dropout = 0.3\n",
    "\n",
    "run_model(LSTM_units = LSTM_units, \n",
    "          FC_units = FC_units, \n",
    "          FC_dropout = FC_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 50, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 50, 512)      1665024     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 50, 512)      2099200     lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 512)          2099200     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4608)         0           lstm_2[0][0]                     \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          2359808     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 13)           6669        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,755,213\n",
      "Trainable params: 8,755,213\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 287s 3s/step - loss: 12.4019 - accuracy: 0.2237 - val_loss: 10.6782 - val_accuracy: 0.3375\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 268s 3s/step - loss: 12.1893 - accuracy: 0.2438 - val_loss: 12.0886 - val_accuracy: 0.2500\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 267s 3s/step - loss: 11.8871 - accuracy: 0.2625 - val_loss: 11.6856 - val_accuracy: 0.2750\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 266s 3s/step - loss: 12.6326 - accuracy: 0.2163 - val_loss: 12.0886 - val_accuracy: 0.2500\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 266s 3s/step - loss: 12.4109 - accuracy: 0.2300 - val_loss: 12.2900 - val_accuracy: 0.2375\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 266s 3s/step - loss: 12.4311 - accuracy: 0.2288 - val_loss: 11.8871 - val_accuracy: 0.2625\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 268s 3s/step - loss: 12.7333 - accuracy: 0.2100 - val_loss: 12.2900 - val_accuracy: 0.2375\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 265s 3s/step - loss: 12.5318 - accuracy: 0.2225 - val_loss: 13.4989 - val_accuracy: 0.1625\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 264s 3s/step - loss: 12.3505 - accuracy: 0.2338 - val_loss: 12.2900 - val_accuracy: 0.2375\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 264s 3s/step - loss: 12.5318 - accuracy: 0.2225 - val_loss: 12.6930 - val_accuracy: 0.2125\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 264s 3s/step - loss: 12.6729 - accuracy: 0.2138 - val_loss: 12.0886 - val_accuracy: 0.2500\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\03 - Visual Question Answering\\vqa_experiments\\LSTMu=512_LSTMd=3_FCu=512_FCd=3_FCdop5_Jan14_12-35-44\\ckpts\\cp_01.ckpt\n",
      "Evaluating the test set...\n",
      "Progress: 0%\n",
      "Progress: 10%\n",
      "Progress: 20%\n",
      "Progress: 30%\n",
      "Progress: 40%\n",
      "Progress: 50%\n",
      "Progress: 60%\n",
      "Progress: 70%\n",
      "Progress: 80%\n",
      "Progress: 90%\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "LSTM_units = [512, 512, 512]\n",
    "FC_units = [512, 512, 512]\n",
    "FC_dropout = 0.5\n",
    "\n",
    "run_model(LSTM_units = LSTM_units,\n",
    "          FC_units = FC_units,\n",
    "          FC_dropout = FC_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2\n",
    "\n",
    "The image is processed with VGG19 to obtain a vector of 4096 values.\n",
    "\n",
    "The question is turned into a vector of 50 GloVe vectors (one per word + padding) and then it's flattened, the result is a vector of 15000 values.\n",
    "\n",
    "The question is then fed to some FC layers (FC1), the output of the last FC1 layer is concatenated with the image vector and this resulting vector is fed to some other FC layers (FC2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 15000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 512)          7680512     input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 512)          0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 4608)         0           dropout_15[0][0]                 \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 512)          2359808     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 512)          0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 512)          262656      dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 512)          0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 13)           6669        dropout_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 10,309,645\n",
      "Trainable params: 10,309,645\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\03 - Visual Question Answering\\vqa_experiments\\FC1_u=512_d=1_do=p3FC2_u=512_d=2_do=p3_Jan14_16-48-50\\ckpts\\cp_04.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x22f95e53348>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the test set...\n",
      "Progress: 0%\n",
      "Progress: 10%\n",
      "Progress: 20%\n",
      "Progress: 30%\n",
      "Progress: 40%\n",
      "Progress: 50%\n",
      "Progress: 60%\n",
      "Progress: 70%\n",
      "Progress: 80%\n",
      "Progress: 90%\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "FC1_units = [512]\n",
    "FC2_units = [512, 512]\n",
    "FC1_dropout = 0.3\n",
    "FC2_dropout = 0.3\n",
    "\n",
    "run_model_alt(FC1_units = FC1_units, FC2_units = FC2_units\n",
    "              FC1_dropout = FC1_dropout, FC2_dropout = FC2_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 15000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         15361024    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          524800      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4608)         0           dropout_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         4719616     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          524800      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 13)           6669        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,136,909\n",
      "Trainable params: 21,136,909\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 428s 4s/step - loss: 12.3046 - accuracy: 0.2300 - val_loss: 12.8945 - val_accuracy: 0.2000\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 396s 4s/step - loss: 12.2095 - accuracy: 0.2425 - val_loss: 11.9878 - val_accuracy: 0.2562\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 393s 4s/step - loss: 12.4009 - accuracy: 0.2306 - val_loss: 12.7937 - val_accuracy: 0.2062\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 382s 4s/step - loss: 12.7736 - accuracy: 0.2075 - val_loss: 12.9952 - val_accuracy: 0.1937\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 377s 4s/step - loss: 12.4613 - accuracy: 0.2269 - val_loss: 11.4841 - val_accuracy: 0.2875\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 377s 4s/step - loss: 12.6829 - accuracy: 0.2131 - val_loss: 12.4915 - val_accuracy: 0.2250\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 371s 4s/step - loss: 12.5822 - accuracy: 0.2194 - val_loss: 12.8945 - val_accuracy: 0.2000\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 383s 4s/step - loss: 12.3606 - accuracy: 0.2331 - val_loss: 12.7937 - val_accuracy: 0.2062\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 385s 4s/step - loss: 12.3203 - accuracy: 0.2356 - val_loss: 13.1967 - val_accuracy: 0.1813\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 376s 4s/step - loss: 12.5721 - accuracy: 0.2200 - val_loss: 12.2900 - val_accuracy: 0.2375\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 372s 4s/step - loss: 12.6124 - accuracy: 0.2175 - val_loss: 12.4915 - val_accuracy: 0.2250\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 380s 4s/step - loss: 12.5721 - accuracy: 0.2200 - val_loss: 12.8945 - val_accuracy: 0.2000\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 379s 4s/step - loss: 12.5923 - accuracy: 0.2188 - val_loss: 11.9878 - val_accuracy: 0.2562\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 362s 4s/step - loss: 12.6729 - accuracy: 0.2138 - val_loss: 12.5923 - val_accuracy: 0.2188\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 356s 4s/step - loss: 12.6023 - accuracy: 0.2181 - val_loss: 12.5923 - val_accuracy: 0.2188\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\03 - Visual Question Answering\\vqa_experiments\\FC1_u=1024_512_do=p5_FC2_u=1024_512__do=p3__Jan14_18-50-32\\ckpts\\cp_05.ckpt\n",
      "Evaluating the test set...\n",
      "Progress: 0%\n",
      "Progress: 10%\n",
      "Progress: 20%\n",
      "Progress: 30%\n",
      "Progress: 40%\n",
      "Progress: 50%\n",
      "Progress: 60%\n",
      "Progress: 70%\n",
      "Progress: 80%\n",
      "Progress: 90%\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "FC1_units = [1024, 512]\n",
    "FC2_units = [1024, 512]\n",
    "FC1_dropout = 0.5\n",
    "FC2_dropout = 0.3\n",
    "\n",
    "run_model_alt(FC1_units = FC1_units, FC2_units = FC2_units,\n",
    "              FC1_dropout = FC1_dropout, FC2_dropout = FC2_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 15000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4096)         61444096    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8192)         0           dropout[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          4194816     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 13)           6669        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 65,908,237\n",
      "Trainable params: 65,908,237\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 400s 4s/step - loss: 13.1513 - accuracy: 0.1762 - val_loss: 12.9952 - val_accuracy: 0.1937\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 380s 4s/step - loss: 13.3579 - accuracy: 0.1713 - val_loss: 12.8945 - val_accuracy: 0.2000\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 375s 4s/step - loss: 13.3377 - accuracy: 0.1725 - val_loss: 14.0026 - val_accuracy: 0.1312\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 377s 4s/step - loss: 12.9448 - accuracy: 0.1969 - val_loss: 12.6930 - val_accuracy: 0.2125\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 370s 4s/step - loss: 13.3075 - accuracy: 0.1744 - val_loss: 13.5996 - val_accuracy: 0.1562\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 367s 4s/step - loss: 13.3277 - accuracy: 0.1731 - val_loss: 13.1967 - val_accuracy: 0.1813\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 365s 4s/step - loss: 13.1463 - accuracy: 0.1844 - val_loss: 13.3982 - val_accuracy: 0.1688\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 362s 4s/step - loss: 13.3881 - accuracy: 0.1694 - val_loss: 13.1967 - val_accuracy: 0.1813\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 362s 4s/step - loss: 13.4385 - accuracy: 0.1663 - val_loss: 13.3982 - val_accuracy: 0.1688\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 363s 4s/step - loss: 13.2874 - accuracy: 0.1756 - val_loss: 13.5996 - val_accuracy: 0.1562\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 362s 4s/step - loss: 13.3780 - accuracy: 0.1700 - val_loss: 13.0960 - val_accuracy: 0.1875\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 361s 4s/step - loss: 13.2168 - accuracy: 0.1800 - val_loss: 12.8945 - val_accuracy: 0.2000\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 364s 4s/step - loss: 13.0859 - accuracy: 0.1881 - val_loss: 13.5996 - val_accuracy: 0.1562\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 361s 4s/step - loss: 13.0355 - accuracy: 0.1912 - val_loss: 13.7004 - val_accuracy: 0.1500\n",
      "Latest model: D:\\Documenti\\GitHub\\A2NDL-HW\\03 - Visual Question Answering\\vqa_experiments\\FC1_u=4096_do=p5_FC2_u=512_512__do=p3__Jan15_00-07-31\\ckpts\\cp_04.ckpt\n",
      "Evaluating the test set...\n",
      "Progress: 0%\n",
      "Progress: 10%\n",
      "Progress: 20%\n",
      "Progress: 30%\n",
      "Progress: 40%\n",
      "Progress: 50%\n",
      "Progress: 60%\n",
      "Progress: 70%\n",
      "Progress: 80%\n",
      "Progress: 90%\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "FC1_units = [4096]\n",
    "FC2_units = [512, 512]\n",
    "FC1_dropout = 0.5\n",
    "FC2_dropout = 0.3\n",
    "\n",
    "run_model_alt(FC1_units = FC1_units, FC2_units = FC2_units,\n",
    "              FC1_dropout = FC1_dropout, FC2_dropout = FC2_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
